{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import config  # âœ… will work if notebook is in same folder as config.py\n",
    "from data.dataset import UnifiedImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.CSV_PATH)\n",
    "\n",
    "dataset = UnifiedImageDataset(\n",
    "    df.rename(columns={\"local_path\": \"file_path\"}),\n",
    "    mode=\"file_df\",\n",
    "    size=config.IMG_SIZE, \n",
    ")\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- sanity preview: first 10 images + saved edge maps ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import config\n",
    "\n",
    "# 2) load saved edge maps (works if dtype is uint8 or float32)\n",
    "edges = np.load(config.EDGE_MAPS_PATH, mmap_mode=\"r\")   # shape: [N, H*W]\n",
    "assert len(dataset) == edges.shape[0], \\\n",
    "    f\"length mismatch: dataset={len(dataset)} vs edges={edges.shape[0]}\"\n",
    "\n",
    "# figure out H,W (fallback to sqrt if config changed)\n",
    "H, W = config.IMG_SIZE\n",
    "if edges.shape[1] != H * W:\n",
    "    side = int(np.sqrt(edges.shape[1]))\n",
    "    H = W = side\n",
    "\n",
    "def _normalize_edge_row(row: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return a 2D edge map in [0,1] no matter the saved dtype.\"\"\"\n",
    "    e = row.reshape(H, W)\n",
    "    if e.dtype == np.uint8:\n",
    "        # typically 0/1 when you saved after (Canny/255.0) into uint8\n",
    "        vmax = 255 if e.max() > 1 else 1\n",
    "        return (e.astype(np.float32) / vmax)\n",
    "    return e.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8081bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- predicted outlines: load model + visualize GT vs Pred ---\n",
    "import config\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.edge_heads import EdgeHead\n",
    "\n",
    "# where you saved the edge head\n",
    "CKPT_PATH = \"best_edge_head.pth\"   # change if needed\n",
    "\n",
    "# embeddings (aligned 1:1 with your CSV/edge maps)\n",
    "EMB_PATH = getattr(config, \"EMBEDDINGS_TARGET_PATH\",\n",
    "           getattr(config, \"EMBEDDINGS_PATH\", None))\n",
    "\n",
    "\n",
    "assert EMB_PATH is not None, \"Please set EMBEDDINGS_TARGET_PATH in config.py\"\n",
    "emb = np.load(EMB_PATH, mmap_mode=\"r\")  # shape: [N, 1024]\n",
    "edges = np.load(config.EDGE_MAPS_PATH, mmap_mode=\"r\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# build & load the model\n",
    "clip_dim = emb.shape[1]\n",
    "edge_dim = edges.shape[1]\n",
    "edge_head = EdgeHead(clip_dim=clip_dim, edge_dim=edge_dim).to(device).eval()\n",
    "edge_head.load_state_dict(torch.load(CKPT_PATH, map_location=device))\n",
    "# --- load the SAME dataset ordering used when you generated the .npy files ---\n",
    "\n",
    "\n",
    "def bce(a, b, eps=1e-7):\n",
    "    \"\"\"Binary cross-entropy between two [0,1] maps (diagnostic only).\"\"\"\n",
    "    a = np.clip(a, eps, 1 - eps)\n",
    "    b = np.clip(b, eps, 1 - eps)\n",
    "    return np.mean(-(a * np.log(b) + (1 - a) * np.log(1 - b)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc28f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- shapes ---\n",
    "H, W = tuple(config.IMG_SIZE)\n",
    "if edges.shape[1] != H * W:          # safeguard if config changed\n",
    "    side = int(np.sqrt(edges.shape[1]))\n",
    "    H = W = side\n",
    "\n",
    "# --- predict one prob-map ---\n",
    "def _predict_edge_map(i: int) -> np.ndarray:\n",
    "    z = torch.from_numpy(emb[i]).float().unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        _, pred_flat, _ = edge_head(z)      # [1, edge_dim], already in [0,1]\n",
    "    return pred_flat.squeeze(0).cpu().numpy().reshape(H, W).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import find_contours\n",
    "from skimage.morphology import binary_opening, remove_small_objects, disk, skeletonize\n",
    "\n",
    "# --- adaptive threshold helpers ---------------------------------------------\n",
    "\n",
    "def topk_threshold(prob: np.ndarray, keep_ratio: float = 0.02) -> float:\n",
    "    \"\"\"\n",
    "    Pick a threshold so that ~keep_ratio of pixels are kept (top-k by prob).\n",
    "    keep_ratio=0.01..0.05 works well for outlines.\n",
    "    \"\"\"\n",
    "    H, W = prob.shape\n",
    "    k = max(1, int(keep_ratio * H * W))\n",
    "    flat = prob.ravel()\n",
    "    # threshold = kth largest value\n",
    "    thr = np.partition(flat, flat.size - k)[flat.size - k]\n",
    "    return float(thr)\n",
    "\n",
    "def clean_and_thin(mask: np.ndarray, min_size: int = 64) -> np.ndarray:\n",
    "    \"\"\"Light morphological cleanup + thinning to get line-like structures.\"\"\"\n",
    "    mask = binary_opening(mask, footprint=disk(1))\n",
    "    mask = remove_small_objects(mask, min_size=min_size)\n",
    "    # Skeletonize to thin the edges to ~1px\n",
    "    mask = skeletonize(mask)\n",
    "    return mask\n",
    "\n",
    "def draw_contour(ax, mask: np.ndarray, color='yellow', lw=2):\n",
    "    \"\"\"Draw contour around a boolean mask.\"\"\"\n",
    "    contours = find_contours(mask.astype(float), level=0.5)\n",
    "    for c in contours:\n",
    "        ax.plot(c[:, 1], c[:, 0], color=color, lw=lw)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "# --- viewer (uses your existing _predict_edge_map, _normalize_edge_row, dataset) ----\n",
    "\n",
    "def show_edges_with_pred(n=10, start=0, fixed_thr=None, keep_ratio=0.02,\n",
    "                         outline_color='yellow', min_size=64):\n",
    "    \"\"\"\n",
    "    If fixed_thr is None, we compute an adaptive threshold that keeps\n",
    "    ~keep_ratio of the most confident pixels.\n",
    "    \"\"\"\n",
    "    end = min(start + n, len(dataset))\n",
    "    for i in range(start, end):\n",
    "        # image\n",
    "        img_t, _ = dataset[i]\n",
    "        img_np = img_t.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        # GT + predicted\n",
    "        gt   = _normalize_edge_row(edges[i])   # (H,W) in [0,1]\n",
    "        pred = _predict_edge_map(i)            # (H,W) in [0,1]\n",
    "        H, W = pred.shape\n",
    "\n",
    "        # choose threshold\n",
    "        thr = fixed_thr if fixed_thr is not None else topk_threshold(pred, keep_ratio=keep_ratio)\n",
    "\n",
    "        # binarize + cleanup\n",
    "        pred_bin = (pred >= thr)\n",
    "        outline_mask = clean_and_thin(pred_bin, min_size=min_size)\n",
    "\n",
    "        # diagnostics\n",
    "        mse = float(np.mean((pred - gt) ** 2))\n",
    "        bce_val = float(bce(gt, pred))\n",
    "        path = dataset.df.iloc[i][\"file_path\"]\n",
    "\n",
    "        # plot\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(16, 3.2))\n",
    "        fig.suptitle(\n",
    "            f\"idx={i} | {path}\\nMSE={mse:.5f}  BCE={bce_val:.5f}  \"\n",
    "            # f\"thr={'auto' if fixed_thr is None else fixed_thr:.2f}  \"\n",
    "            f\"(keep={keep_ratio*100:.1f}%)\",\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "        ax[0].imshow(img_np);                 ax[0].set_title(\"Image\"); ax[0].axis(\"off\")\n",
    "        ax[1].imshow(gt, cmap=\"gray\", vmin=0, vmax=1); ax[1].set_title(\"GT edge\"); ax[1].axis(\"off\")\n",
    "        ax[2].imshow(pred, cmap=\"gray\", vmin=0, vmax=1); ax[2].set_title(\"Pred heatmap\"); ax[2].axis(\"off\")\n",
    "\n",
    "        ax[3].imshow(img_np)\n",
    "        ax[3].imshow(pred, cmap=\"magma\", vmin=0, vmax=1, alpha=0.30)\n",
    "        draw_contour(ax[3], outline_mask, color=outline_color, lw=2)\n",
    "        ax[3].set_title(\"Pred overlay\"); ax[3].axis(\"off\")\n",
    "\n",
    "        ax[4].imshow(img_np)\n",
    "        # visualize the binary mask softly so the image is visible below\n",
    "        ax[4].imshow(outline_mask, cmap=\"autumn\", vmin=0, vmax=1, alpha=0.45)\n",
    "        ax[4].set_title(\"Pred outline\"); ax[4].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddc051",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_edges_with_pred(n=20, start=0, fixed_thr=None, keep_ratio=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c4ba3",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5d01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HF_CACHE = \"/data/hf-cache\"\n",
    "os.makedirs(HF_CACHE, exist_ok=True)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = HF_CACHE\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = os.path.join(HF_CACHE, \"hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"]     = os.path.join(HF_CACHE, \"transformers\")\n",
    "os.environ[\"DIFFUSERS_CACHE\"]        = os.path.join(HF_CACHE, \"diffusers\")\n",
    "os.environ[\"TORCH_HOME\"]             = os.path.join(HF_CACHE, \"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_head.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from torchvision import transforms\n",
    "from ip_adapter import IPAdapter\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "def display_images(images):\n",
    "    for img in images:\n",
    "        display(img)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_from_dataset_id(i: int,\n",
    "                             prompt: str = \"a cat playing with a ball\",\n",
    "                             guidance_scale: float = 10.0,\n",
    "                             steps: int = 50):\n",
    "    # 1) Get the original image from your dataset\n",
    "    img_t, _ = dataset[i]\n",
    "    pil_img = to_pil(img_t)\n",
    "\n",
    "    # 2) Get a CLIP image embedding\n",
    "    # Prefer precomputed `emb[i]`; fall back to computing via clip_model+preprocess\n",
    "    try:\n",
    "        z_clip = torch.as_tensor(emb[i], dtype=torch.float32, device=device).unsqueeze(0)  # [1, D]\n",
    "    except Exception:\n",
    "        assert 'clip_model' in globals() and 'preprocess' in globals(), \\\n",
    "            \"Need `clip_model` and `preprocess` to compute CLIP on the fly.\"\n",
    "        z_clip = clip_model.encode_image(preprocess(pil_img).unsqueeze(0).to(device)).float()\n",
    "\n",
    "\n",
    "    _, _, edge_map_embedding = edge_head(z_clip)  # keep if you want to inspect/log it\n",
    "\n",
    "    # 4) Generate from CLIP embedding with IP-Adapter\n",
    "    images = ip_adapter.generate_from_embeddings(\n",
    "        clip_image_embeds=edge_map_embedding,             # <-- use CLIP embedding\n",
    "        prompt=prompt,\n",
    "        num_samples=1,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=steps,            # 30â€“60 is usually plenty\n",
    "    )\n",
    "\n",
    "    # 5) Display: original then generated\n",
    "    display(pil_img)\n",
    "    display_images(images)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f373634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_dataset_id(i=1,\n",
    "                         prompt=\"a cat\",\n",
    "                         guidance_scale=7.5,\n",
    "                         steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba34b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Initialize Stable Diffusion pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,   # saves ~1.2 GB locally\n",
    "    feature_extractor=None,\n",
    "    cache_dir=HF_CACHE,\n",
    ").to(device)\n",
    "\n",
    "# Initialize IP-Adapter with custom embedding type\n",
    "ip_adapter = IPAdapter(\n",
    "    sd_pipe=pipe,\n",
    "    image_encoder_path=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n",
    "    ip_ckpt=\"/data/thesis/models/ip-adapter_sd15.bin\",\n",
    "    device=device,\n",
    "    embedding_type='clip'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
