{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Generation with IP-Adapter (SD 1.5)\n",
        "\n",
        "This notebook demonstrates how to use the DEGIS package to:\n",
        "1. Load trained color head models\n",
        "2. Set up IP-Adapter with ControlNet for image generation\n",
        "3. Generate images using color and layout control\n",
        "\n",
        "Based on the ablation notebook but using the new package structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install IP-Adapter and dependencies\n",
        "%pip uninstall -y ip-adapter diffusers\n",
        "%pip install --no-cache-dir git+https://github.com/Ahmed-Sherif-ASA/IP-Adapter@main\n",
        "%pip install diffusers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from IPython.display import display\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Import the DEGIS package\n",
        "import degis\n",
        "from degis.data.dataset import UnifiedImageDataset\n",
        "from degis.config import CSV_PATH, HF_XL_EMBEDDINGS_TARGET_PATH, COLOR_HIST_PATH_HCL_514, EDGE_MAPS_PATH\n",
        "\n",
        "# Import IP-Adapter\n",
        "import ip_adapter\n",
        "from ip_adapter import IPAdapter\n",
        "from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data and Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "colour_dataset = UnifiedImageDataset(\n",
        "    df.rename(columns={\"local_path\": \"file_path\"}),\n",
        "    mode=\"file_df\",\n",
        "    size=(224, 224),\n",
        "    subset_ratio=1.0\n",
        ")\n",
        "\n",
        "# Load precomputed data\n",
        "embeddings = np.load(HF_XL_EMBEDDINGS_TARGET_PATH, mmap_mode=\"r\").astype(np.float32, copy=False)\n",
        "histograms = np.load(COLOR_HIST_PATH_HCL_514, mmap_mode=\"r\").astype(np.float32, copy=False)\n",
        "edge_maps = np.load(EDGE_MAPS_PATH, mmap_mode=\"r\")\n",
        "\n",
        "print(f\"Loaded embeddings: {embeddings.shape}\")\n",
        "print(f\"Loaded histograms: {histograms.shape}\")\n",
        "print(f\"Loaded edge maps: {edge_maps.shape}\")\n",
        "\n",
        "# Find the latest trained model\n",
        "run_dirs = glob.glob(\"runs/*\")\n",
        "if run_dirs:\n",
        "    latest_run = max(run_dirs, key=os.path.getctime)\n",
        "    checkpoint_path = os.path.join(latest_run, \"best_color_head_tmp.pth\")\n",
        "    print(f\"Using checkpoint: {checkpoint_path}\")\n",
        "else:\n",
        "    # Fallback to a default path\n",
        "    checkpoint_path = \"best_color_head.pth\"\n",
        "    print(f\"Using default checkpoint: {checkpoint_path}\")\n",
        "\n",
        "# Load trained color head\n",
        "color_head = degis.load_trained_color_head(\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    clip_dim=embeddings.shape[1],\n",
        "    hist_dim=histograms.shape[1],\n",
        "    device=device\n",
        ")\n",
        "print(\"✓ Color head loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Setup IP-Adapter Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup cache directory\n",
        "HF_CACHE = \"/data/hf-cache\" if os.path.exists(\"/data\") else \"./hf-cache\"\n",
        "os.makedirs(HF_CACHE, exist_ok=True)\n",
        "\n",
        "os.environ[\"HF_HOME\"] = HF_CACHE\n",
        "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = os.path.join(HF_CACHE, \"hub\")\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(HF_CACHE, \"transformers\")\n",
        "os.environ[\"DIFFUSERS_CACHE\"] = os.path.join(HF_CACHE, \"diffusers\")\n",
        "os.environ[\"TORCH_HOME\"] = os.path.join(HF_CACHE, \"torch\")\n",
        "\n",
        "print(f\"Using cache directory: {HF_CACHE}\")\n",
        "\n",
        "# Create IP-Adapter generator\n",
        "generator = degis.IPAdapterGenerator(device=device)\n",
        "\n",
        "# Setup the pipeline\n",
        "generator.setup_pipeline(\n",
        "    model_id=\"runwayml/stable-diffusion-v1-5\",\n",
        "    controlnet_id=\"lllyasviel/control_v11p_sd15_canny\",\n",
        "    ip_ckpt=\"/data/thesis/models/ip-adapter_sd15.bin\",  # Update path as needed\n",
        "    image_encoder_path=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n",
        "    cache_dir=HF_CACHE,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "print(\"✓ IP-Adapter pipeline setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Image Generation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_from_dataset_id(\n",
        "    colour_index: int,\n",
        "    layout_index: int,\n",
        "    prompt: str = \"a cat playing with a ball\",\n",
        "    guidance_scale: float = 7.5,\n",
        "    steps: int = 30,\n",
        "    controlnet_conditioning_scale: float = 1.0,\n",
        "    num_samples: int = 1,\n",
        "    scale: float = 0.8,\n",
        "):\n",
        "    \"\"\"Generate images using color and layout control.\"\"\"\n",
        "    \n",
        "    # Get original image for display\n",
        "    img_t, _ = colour_dataset[colour_index]\n",
        "    pil_img = transforms.ToPILImage()(img_t)\n",
        "    \n",
        "    # Get CLIP embedding and compute color embedding\n",
        "    z_clip = torch.as_tensor(embeddings[colour_index], dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    color_embedding = degis.get_color_embedding(color_head, z_clip)\n",
        "    \n",
        "    # Create control image from edge data\n",
        "    control_image = degis.create_edge_control_image(edge_maps[layout_index], size=512)\n",
        "    \n",
        "    # Generate images\n",
        "    images = generator.generate(\n",
        "        color_embedding=color_embedding,\n",
        "        control_image=control_image,\n",
        "        prompt=prompt,\n",
        "        negative_prompt=(\n",
        "            \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry, \"\n",
        "            \"sketch, cartoon, drawing, anime:1.4, comic, illustration, posterized, \"\n",
        "            \"mosaic, stained glass, abstract, surreal, psychedelic, trippy, texture artifact, \"\n",
        "            \"embroidery, knitted, painting, oversaturated, unrealistic, bad shading\"\n",
        "        ),\n",
        "        num_samples=num_samples,\n",
        "        guidance_scale=guidance_scale,\n",
        "        num_inference_steps=steps,\n",
        "        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
        "        scale=scale,\n",
        "    )\n",
        "    \n",
        "    # Display results\n",
        "    comparison = degis.display_comparison_grid(\n",
        "        original=pil_img,\n",
        "        control=control_image,\n",
        "        generated=images,\n",
        "        cols=3\n",
        "    )\n",
        "    display(comparison)\n",
        "    \n",
        "    return images\n",
        "\n",
        "print(\"✓ Generation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generate Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate images with different prompts and parameters\n",
        "print(\"Generating images with IP-Adapter (SD 1.5)...\")\n",
        "\n",
        "# Example 1: Cat with ball\n",
        "images1 = generate_from_dataset_id(\n",
        "    colour_index=1000,\n",
        "    layout_index=33,\n",
        "    prompt=\"a cat playing with a ball\",\n",
        "    guidance_scale=7.5,\n",
        "    steps=30,\n",
        "    controlnet_conditioning_scale=1.0,\n",
        "    num_samples=1,\n",
        "    scale=0.8,\n",
        ")\n",
        "\n",
        "# Example 2: Dog on hoodie\n",
        "images2 = generate_from_dataset_id(\n",
        "    colour_index=1008,\n",
        "    layout_index=33,\n",
        "    prompt=\"a dog on the hoodie\",\n",
        "    guidance_scale=7.5,\n",
        "    steps=30,\n",
        "    controlnet_conditioning_scale=1.0,\n",
        "    num_samples=1,\n",
        "    scale=0.8,\n",
        ")\n",
        "\n",
        "# Example 3: Different style\n",
        "images3 = generate_from_dataset_id(\n",
        "    colour_index=1003,\n",
        "    layout_index=33,\n",
        "    prompt=\"A cat on the hoodie, digital art style\",\n",
        "    guidance_scale=13.0,\n",
        "    steps=50,\n",
        "    controlnet_conditioning_scale=0.8,\n",
        "    num_samples=1,\n",
        "    scale=0.6,\n",
        ")\n",
        "\n",
        "print(\"✓ Image generation complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
