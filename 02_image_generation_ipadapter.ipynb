{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Generation with IP-Adapter (SD 1.5)\n",
        "\n",
        "This notebook demonstrates how to use the DEGIS package to:\n",
        "1. Load trained color head models\n",
        "2. Set up IP-Adapter with ControlNet for image generation\n",
        "3. Generate images using color and layout control\n",
        "\n",
        "Based on the ablation notebook but using the new package structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Install IP-Adapter and dependencies\n",
        "# %pip uninstall -y ip-adapter diffusers\n",
        "# %pip install --no-cache-dir git+https://github.com/Ahmed-Sherif-ASA/IP-Adapter@main\n",
        "# %pip install diffusers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from IPython.display import display\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Import the DEGIS package\n",
        "import degis\n",
        "from degis.data.dataset import UnifiedImageDataset\n",
        "from degis.config import CSV_PATH, EMBEDDINGS_TARGET_PATH, COLOR_HIST_PATH_HCL_514, EDGE_MAPS_PATH\n",
        "\n",
        "# Import IP-Adapter\n",
        "import ip_adapter\n",
        "from ip_adapter import IPAdapter\n",
        "from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data and Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_path = EMBEDDINGS_TARGET_PATH # || YOUR_CUSTOM_PATH\n",
        "colour_path = COLOR_HIST_PATH_HCL_514 # options: COLOR_HIST_PATH_LAB_514 || COLOR_HIST_PATH_RGB || YOUR_CUSTOM_PATH\n",
        "edge_maps_path = EDGE_MAPS_PATH # || YOUR_CUSTOM_PATH\n",
        "colour_head_checkpoint_path = \"/data/degis/runs/hcl514_tk100_b4096-20250827-222013/best_color_head_tmp_hcl.pth\" # || YOUR_CUSTOM_PATH\n",
        "ip_ckpt = \"/data/thesis/models/ip-adapter_sd15.bin\"\n",
        "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
        "controlnet_id = \"lllyasviel/control_v11p_sd15_canny\"\n",
        "stable_diffusion_model_id = \"runwayml/stable-diffusion-v1-5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded embeddings: (3336240, 1024)\n",
            "Loaded histograms: (3336240, 514)\n",
            "Loaded edge maps: (3336240, 50176)\n",
            "✓ Color head loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "colour_dataset = UnifiedImageDataset(\n",
        "    df.rename(columns={\"local_path\": \"file_path\"}),\n",
        "    mode=\"file_df\",\n",
        "    size=(224, 224),\n",
        "    subset_ratio=1.0\n",
        ")\n",
        "\n",
        "# Load precomputed data\n",
        "embeddings = np.load(embeddings_path, mmap_mode=\"r\").astype(np.float32, copy=False)\n",
        "histograms = np.load(colour_path, mmap_mode=\"r\").astype(np.float32, copy=False)\n",
        "edge_maps = np.load(edge_maps_path, mmap_mode=\"r\")\n",
        "\n",
        "print(f\"Loaded embeddings: {embeddings.shape}\")\n",
        "print(f\"Loaded histograms: {histograms.shape}\")\n",
        "print(f\"Loaded edge maps: {edge_maps.shape}\")\n",
        "\n",
        "# Load trained color head\n",
        "color_head = degis.load_trained_color_head(\n",
        "    checkpoint_path=colour_head_checkpoint_path,\n",
        "    clip_dim=embeddings.shape[1],\n",
        "    hist_dim=histograms.shape[1],\n",
        "    device=device\n",
        ")\n",
        "print(\"✓ Color head loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Setup IP-Adapter Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cache directory: /data/hf-cache\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8181ebc79e314c10abb01b09a4c2d05d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ IP-Adapter pipeline setup complete\n"
          ]
        }
      ],
      "source": [
        "# Setup cache directory\n",
        "HF_CACHE = \"/data/hf-cache\" if os.path.exists(\"/data\") else \"./hf-cache\"\n",
        "os.makedirs(HF_CACHE, exist_ok=True)\n",
        "\n",
        "os.environ[\"HF_HOME\"] = HF_CACHE\n",
        "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = os.path.join(HF_CACHE, \"hub\")\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(HF_CACHE, \"transformers\")\n",
        "os.environ[\"DIFFUSERS_CACHE\"] = os.path.join(HF_CACHE, \"diffusers\")\n",
        "os.environ[\"TORCH_HOME\"] = os.path.join(HF_CACHE, \"torch\")\n",
        "\n",
        "print(f\"Using cache directory: {HF_CACHE}\")\n",
        "\n",
        "# Create IP-Adapter generator\n",
        "generator = degis.IPAdapterGenerator(device=device)\n",
        "\n",
        "# Setup the pipeline\n",
        "generator.setup_pipeline(\n",
        "    model_id=stable_diffusion_model_id,\n",
        "    controlnet_id=controlnet_id,\n",
        "    ip_ckpt=ip_ckpt,\n",
        "    image_encoder_path=image_encoder_path,\n",
        "    cache_dir=HF_CACHE,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "print(\"✓ IP-Adapter pipeline setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Image Generation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Generation function defined\n"
          ]
        }
      ],
      "source": [
        "def generate_from_dataset_id(\n",
        "    colour_index: int,\n",
        "    layout_index: int,\n",
        "    prompt: str = \"a cat playing with a ball\",\n",
        "    guidance_scale: float = 7.5,\n",
        "    steps: int = 30,\n",
        "    controlnet_conditioning_scale: float = 1.0,\n",
        "    num_samples: int = 1,\n",
        "    scale: float = 0.8,\n",
        "):\n",
        "    \"\"\"Generate images using color and layout control.\"\"\"\n",
        "    \n",
        "    # Get original image for display\n",
        "    img_t, _ = colour_dataset[colour_index]\n",
        "    pil_img = transforms.ToPILImage()(img_t)\n",
        "    \n",
        "    # Get CLIP embedding and compute color embedding\n",
        "    z_clip = torch.as_tensor(embeddings[colour_index], dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    color_embedding = degis.get_color_embedding(color_head, z_clip)\n",
        "    \n",
        "    # Create control image from edge data\n",
        "    control_image = degis.create_edge_control_image(edge_maps[layout_index], size=512)\n",
        "    \n",
        "    # Generate images\n",
        "    images = generator.generate(\n",
        "        color_embedding=color_embedding,\n",
        "        control_image=control_image,\n",
        "        prompt=prompt,\n",
        "        negative_prompt=(\n",
        "            \"monochrome, lowres, bad anatomy, worst quality, low quality, blurry, \"\n",
        "            \"sketch, cartoon, drawing, anime:1.4, comic, illustration, posterized, \"\n",
        "            \"mosaic, stained glass, abstract, surreal, psychedelic, trippy, texture artifact, \"\n",
        "            \"embroidery, knitted, painting, oversaturated, unrealistic, bad shading\"\n",
        "        ),\n",
        "        num_samples=num_samples,\n",
        "        guidance_scale=guidance_scale,\n",
        "        num_inference_steps=steps,\n",
        "        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
        "        scale=scale,\n",
        "    )\n",
        "    \n",
        "    # Display results\n",
        "    comparison = degis.display_comparison_grid(\n",
        "        original=pil_img,\n",
        "        control=control_image,\n",
        "        generated=images,\n",
        "        cols=3\n",
        "    )\n",
        "    display(comparison)\n",
        "    \n",
        "    return images\n",
        "\n",
        "print(\"✓ Generation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generate Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating images with IP-Adapter (SD 1.5)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_717251/429353600.py:18: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  z_clip = torch.as_tensor(embeddings[colour_index], dtype=torch.float32, device=device).unsqueeze(0)\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'degis' has no attribute 'create_edge_control_image'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating images with IP-Adapter (SD 1.5)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Example 1: Cat with ball\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m images1 = \u001b[43mgenerate_from_dataset_id\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolour_index\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayout_index\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m33\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma cat playing with a ball\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_conditioning_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Example 2: Dog on hoodie\u001b[39;00m\n\u001b[32m     17\u001b[39m images2 = generate_from_dataset_id(\n\u001b[32m     18\u001b[39m     colour_index=\u001b[32m1008\u001b[39m,\n\u001b[32m     19\u001b[39m     layout_index=\u001b[32m33\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     scale=\u001b[32m0.8\u001b[39m,\n\u001b[32m     26\u001b[39m )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mgenerate_from_dataset_id\u001b[39m\u001b[34m(colour_index, layout_index, prompt, guidance_scale, steps, controlnet_conditioning_scale, num_samples, scale)\u001b[39m\n\u001b[32m     19\u001b[39m color_embedding = degis.get_color_embedding(color_head, z_clip)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Create control image from edge data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m control_image = \u001b[43mdegis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_edge_control_image\u001b[49m(edge_maps[layout_index], size=\u001b[32m512\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Generate images\u001b[39;00m\n\u001b[32m     25\u001b[39m images = generator.generate(\n\u001b[32m     26\u001b[39m     color_embedding=color_embedding,\n\u001b[32m     27\u001b[39m     control_image=control_image,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     scale=scale,\n\u001b[32m     40\u001b[39m )\n",
            "\u001b[31mAttributeError\u001b[39m: module 'degis' has no attribute 'create_edge_control_image'"
          ]
        }
      ],
      "source": [
        "# Generate images with different prompts and parameters\n",
        "print(\"Generating images with IP-Adapter (SD 1.5)...\")\n",
        "\n",
        "# Example 1: Cat with ball\n",
        "images1 = generate_from_dataset_id(\n",
        "    colour_index=1000,\n",
        "    layout_index=33,\n",
        "    prompt=\"a cat playing with a ball\",\n",
        "    guidance_scale=7.5,\n",
        "    steps=30,\n",
        "    controlnet_conditioning_scale=1.0,\n",
        "    num_samples=1,\n",
        "    scale=0.8,\n",
        ")\n",
        "\n",
        "# Example 2: Dog on hoodie\n",
        "images2 = generate_from_dataset_id(\n",
        "    colour_index=1008,\n",
        "    layout_index=33,\n",
        "    prompt=\"a dog on the hoodie\",\n",
        "    guidance_scale=7.5,\n",
        "    steps=30,\n",
        "    controlnet_conditioning_scale=1.0,\n",
        "    num_samples=1,\n",
        "    scale=0.8,\n",
        ")\n",
        "\n",
        "# Example 3: Different style\n",
        "images3 = generate_from_dataset_id(\n",
        "    colour_index=1003,\n",
        "    layout_index=33,\n",
        "    prompt=\"A cat on the hoodie, digital art style\",\n",
        "    guidance_scale=13.0,\n",
        "    steps=50,\n",
        "    controlnet_conditioning_scale=0.8,\n",
        "    num_samples=1,\n",
        "    scale=0.6,\n",
        ")\n",
        "\n",
        "print(\"✓ Image generation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
