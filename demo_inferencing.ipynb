{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655ec24d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"docs/inference_sequence_diagram.png\" alt=\"inferencing diagram\" width=\"1200\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9d352",
   "metadata": {},
   "source": [
    "# TITLE & OVERVIEW\n",
    "\n",
    "This notebook runs DEGIS image generation with two modes: Style (Base Ip-Adapter) and Sinkhorn-constrained (DEGIS Approach) color alignment.  \n",
    "The notebook builds a hypothetical scenario where a car dealer, interested in creating advertisements, that follows predefined color and layout.\n",
    "We evaluate three edge map and for each we use three color sources.  \n",
    "\n",
    "Each evaluation section shows: the source color image, the chosen edge map, a style result, and an Sinkhorn-constrained result with metrics.\n",
    "\n",
    "## ASSETS EXPECTED\n",
    "- `edge_maps` and a trained `color_head`.  \n",
    "- A ready `generator` pipeline.\n",
    "\n",
    "## OUTPUTS OF INTEREST\n",
    "- One visualization image per run.  \n",
    "- Metrics: **Sinkhorn** (lower is better), **CLIP-text cosine** (higher is better), **attempts**, and **generation time**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97525881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT CHECK - Run this cell first!\n",
    "# =============================================================================\n",
    "# This cell verifies that the DEGIS environment is properly set up\n",
    "# Make sure you've run ./setup_server_fixed.sh first!\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if we're in the right environment\n",
    "if 'degis-env' in sys.executable:\n",
    "    print(\"DEGIS environment is active\")\n",
    "    print(f\"Python: {sys.executable}\")\n",
    "else:\n",
    "    print(\"Warning: DEGIS environment not detected\")\n",
    "    print(\"Please run: ./setup_server_fixed.sh\")\n",
    "    print(\"Then activate: source degis-env/bin/activate\")\n",
    "\n",
    "# Check if DEGIS package is available\n",
    "try:\n",
    "    import degis\n",
    "    print(\"DEGIS package is available\")\n",
    "except ImportError:\n",
    "    print(\"DEGIS package not found\")\n",
    "    print(\"Please run: ./setup_server_fixed.sh\")\n",
    "\n",
    "print(\"\\nReady to start image generation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df66cc",
   "metadata": {},
   "source": [
    "# IMPORTS - CONTEXT\n",
    "\n",
    "Loads core libs (torch, numpy, PIL, matplotlib, torchvision), DEGIS dataset helpers, SDXL + ControlNet with DEGIS patches, and histogram utilities.  \n",
    "Prints the active device (`cuda` or `cpu`). No parameters are changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db71031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geomloss\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from IPython.display import display\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Import DEGIS package components\n",
    "from degis import IPAdapterXLGenerator, load_trained_color_head, get_color_embedding\n",
    "from degis.shared.utils import plot_color_palette, display_images_grid, display_comparison_grid\n",
    "from degis.shared.image_features.color_histograms import compute_color_histogram, compute_lab_histogram\n",
    "from degis.shared.utils.image_utils import create_control_edge_pil\n",
    "from degis.data.dataset import UnifiedImageDataset\n",
    "\n",
    "# Import IP-Adapter XL with DEGIS patches\n",
    "import ip_adapter_patch  # This applies the DEGIS patches\n",
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(\"Ready for high-quality image generation with IP-Adapter XL!\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d18da",
   "metadata": {},
   "source": [
    "# PATHS & GLOBAL CONFIG\n",
    "\n",
    "CSV manifest, edge maps `.npy`, and the **color head checkpoint** paths.  \n",
    "Define the **image encoder** and **ControlNet** identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61365ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"data/laion_5m_manifest.csv\"\n",
    "color_head_checkpoint_path = \"evaluation_runs/laion_5m_xl_lab514_tk20_b4096/best_color_head_tmp.pth\"\n",
    "precomputed_adimagenet_edge_maps_path = \"data/adimagenet_edge_maps.npy\"\n",
    "\n",
    "image_encoder_path = \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\"\n",
    "controlnet_id = \"diffusers/controlnet-canny-sdxl-1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4927f943",
   "metadata": {},
   "source": [
    "# LOAD DATASETS & ARRAYS - EXPECTED SHAPES\n",
    "\n",
    "Loads the thumbnail dataset (for selecting `color_index`), memory-maps large arrays for **embeddings**, **histograms**, and **edge maps**, and loads the trained **color head**.  \n",
    "Expect printed shapes for sanity checking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from degis.inference import load_trained_color_head\n",
    "# Load datasets\n",
    "df = pd.read_csv(csv_path)\n",
    "color_dataset = UnifiedImageDataset(\n",
    "    df.rename(columns={\"local_path\": \"file_path\"}),\n",
    "    mode=\"url_df\",\n",
    "    size=(224, 224),\n",
    "    subset_ratio=1.0\n",
    ")\n",
    "\n",
    "# Load precomputed data\n",
    "edge_maps = np.load(precomputed_adimagenet_edge_maps_path, mmap_mode=\"r\")\n",
    "\n",
    "print(f\"Loaded edge maps: {edge_maps.shape}\")\n",
    "\n",
    "# Load trained color head\n",
    "color_head = load_trained_color_head(\n",
    "    checkpoint_path=color_head_checkpoint_path,\n",
    "    clip_dim=1280,\n",
    "    hist_dim=514,\n",
    "    device=device\n",
    ")\n",
    "print(\"Color head loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c68d80",
   "metadata": {},
   "source": [
    "# HF CACHE SETUP & INITIALIZE PIPELINE\n",
    "\n",
    "Sets local caches for Hugging Face components and initializes the IP-Adapter XL pipeline.  \n",
    "First run may download weights; reruns use the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cache directory\n",
    "HF_CACHE = \"/data/hf-cache\" if os.path.exists(\"/data\") else \"./hf-cache\"\n",
    "os.makedirs(HF_CACHE, exist_ok=True)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = HF_CACHE\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = os.path.join(HF_CACHE, \"hub\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(HF_CACHE, \"transformers\")\n",
    "os.environ[\"DIFFUSERS_CACHE\"] = os.path.join(HF_CACHE, \"diffusers\")\n",
    "os.environ[\"TORCH_HOME\"] = os.path.join(HF_CACHE, \"torch\")\n",
    "\n",
    "print(f\"Using cache directory: {HF_CACHE}\")\n",
    "\n",
    "# Create IP-Adapter XL generator\n",
    "generator = IPAdapterXLGenerator(device=device)\n",
    "\n",
    "# Setup the pipeline\n",
    "generator.setup_pipeline(\n",
    "    model_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet_id=controlnet_id,\n",
    "    ip_ckpt=None,  # Set to None to use default IP-Adapter weights\n",
    "    image_encoder_path=image_encoder_path,\n",
    "    cache_dir=HF_CACHE,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(\"IP-Adapter XL pipeline setup complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf92da",
   "metadata": {},
   "source": [
    "# GENERATION FUNCTION - STYLE VS Sinkhorn\n",
    "\n",
    "Reusable function to: clear VRAM, build control image, compute target histogram, run **Style** generation, run **Sinkhorn-constrained** generation, compute metrics, and render a composite visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e71ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "from degis.inference import generate_by_style, generate_by_color_sinkhorn_constrained\n",
    "from degis.shared.utils.visualization import visualize_generation_comparison, create_generation_metrics\n",
    "from degis.shared.utils.image_utils import create_control_edge_pil\n",
    "from degis.inference.core_generation import get_color_embedding\n",
    "\n",
    "def generate_comparison_snippet(\n",
    "    color_index: int,\n",
    "    layout_index: int,\n",
    "    prompt: str = \"a cat playing with a ball\",\n",
    "    guidance_scale: float = 6.5,\n",
    "    steps: int = 40,\n",
    "    controlnet_conditioning_scale: float = 0.8,\n",
    "    num_samples: int = 1,\n",
    "    attn_ip_scale: float = 0.8,\n",
    "    text_token_scale: float = 1.0,\n",
    "    ip_token_scale: float = None,\n",
    "    ip_uncond_scale: float = 0.0,\n",
    "    zero_ip_in_uncond: bool = True,\n",
    "    # Sinkhorn generation parameters\n",
    "    target_sinkhorn_threshold: float = 0.1,\n",
    "    max_attempts: int = 20,\n",
    "    top_k: int = 20,\n",
    "    color_space: str = None,\n",
    "):\n",
    "    \"\"\"Simple snippet to generate both style and Sinkhorn images with visualization.\"\"\"\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Generating comparison for color_index={color_index}, layout_index={layout_index}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    \n",
    "    # Get original image for display\n",
    "    img_t, _ = color_dataset[color_index]\n",
    "    pil_img = transforms.ToPILImage()(img_t)\n",
    "    \n",
    "    # Compute CLIP embedding\n",
    "    from degis.shared.clip_vit_bigg14 import compute_clip_embedding_xl\n",
    "    z_clip = compute_clip_embedding_xl(pil_img).to(device).unsqueeze(0)\n",
    "    color_embedding = get_color_embedding(color_head, z_clip)\n",
    "    \n",
    "    # Create control image from edge data\n",
    "    control_image = create_control_edge_pil(edge_maps[layout_index], size=512)\n",
    "    \n",
    "    # Compute color histogram for Sinkhorn constraint\n",
    "    from degis.inference.generation_functions import compute_histogram_for_color_space, detect_color_space\n",
    "    color_histogram = compute_histogram_for_color_space(pil_img, color_space or \"lab\", bins=8)\n",
    "    if color_space is None:\n",
    "        color_space = detect_color_space(color_histogram)\n",
    "    \n",
    "    print(f\"Color space: {color_space}, Histogram shape: {color_histogram.shape}\")\n",
    "    \n",
    "    # Edge detection helper functions (from the run_eval_pairs)\n",
    "    def _canny_bool(pil_img, sigma=1.0):\n",
    "        import numpy as np\n",
    "        try:\n",
    "            from skimage.color import rgb2gray\n",
    "            from skimage.feature import canny\n",
    "            g = rgb2gray(np.asarray(pil_img).astype(np.float32) / 255.0)\n",
    "            e = canny(g, sigma=sigma)\n",
    "            return e.astype(np.uint8)\n",
    "        except Exception:\n",
    "            from PIL import ImageFilter\n",
    "            e = pil_img.convert(\"L\").filter(ImageFilter.FIND_EDGES)\n",
    "            return (np.asarray(e) > 32).astype(np.uint8)\n",
    "\n",
    "    def _edge_scores(pred_bool, ref_bool):\n",
    "        import numpy as np\n",
    "        pred = pred_bool.astype(bool); ref = ref_bool.astype(bool)\n",
    "        tp = np.logical_and(pred, ref).sum()\n",
    "        fp = np.logical_and(pred, ~ref).sum()\n",
    "        fn = np.logical_and(~pred, ref).sum()\n",
    "        prec = tp / (tp + fp + 1e-9)\n",
    "        rec  = tp / (tp + fn + 1e-9)\n",
    "        f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "        iou  = tp / (tp + fp + fn + 1e-9)\n",
    "        try:\n",
    "            from skimage.metrics import structural_similarity as ssim\n",
    "            ssimv = ssim(pred_bool.astype(np.float32), ref_bool.astype(np.float32))\n",
    "        except Exception:\n",
    "            ssimv = np.nan\n",
    "        return f1, iou, ssimv\n",
    "    \n",
    "    # Get reference edge map for IoU calculation\n",
    "    control_bool = (np.asarray(control_image.convert(\"L\")) > 0).astype(np.uint8)\n",
    "    \n",
    "    # 1. Style Generation (one-shot)\n",
    "    print(\"\\nRunning style generation...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    style_images = generate_by_style(\n",
    "        generator=generator,\n",
    "        pil_image=pil_img,\n",
    "        control_image=control_image,\n",
    "        prompt=prompt,\n",
    "        num_samples=num_samples,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=steps,\n",
    "        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "        attn_ip_scale=attn_ip_scale,\n",
    "        text_token_scale=text_token_scale,\n",
    "        ip_token_scale=ip_token_scale,\n",
    "        ip_uncond_scale=ip_uncond_scale,\n",
    "        zero_ip_in_uncond=zero_ip_in_uncond\n",
    "    )\n",
    "    style_time = time.time() - start_time\n",
    "    style_generated_image = style_images[0]\n",
    "    \n",
    "    # Calculate metrics for style generation\n",
    "    from degis.inference.generation_functions import calculate_sinkhorn_distance_topk, calculate_cosine_similarity\n",
    "    style_sinkhorn = calculate_sinkhorn_distance_topk(color_histogram, compute_histogram_for_color_space(style_generated_image, color_space, bins=8))\n",
    "    style_cosine = calculate_cosine_similarity(prompt, style_generated_image)\n",
    "    \n",
    "    # Calculate edge metrics for style\n",
    "    f1_s, iou_s, ssim_s = _edge_scores(_canny_bool(style_generated_image), control_bool)\n",
    "    \n",
    "    style_metrics = {\n",
    "        'generation_time': f\"{style_time:.2f}\",\n",
    "        'sinkhorn': style_sinkhorn,\n",
    "        'cosine': style_cosine,\n",
    "        'iou': iou_s\n",
    "    }\n",
    "    \n",
    "    print(f\"Style generation: {style_time:.2f}s, Sinkhorn: {style_sinkhorn:.4f}, Cosine: {style_cosine:.4f}, IoU: {iou_s:.4f}\")\n",
    "    \n",
    "    # 2. Sinkhorn-Constrained Generation\n",
    "    print(\"\\nRunning Sinkhorn-constrained generation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sinkhorn_images, sinkhorn_score, sinkhorn_cosine, attempts = generate_by_color_sinkhorn_constrained(\n",
    "        generator=generator,\n",
    "        color_embedding=color_embedding,\n",
    "        control_image=control_image,\n",
    "        original_histogram=color_histogram,\n",
    "        prompt=prompt,\n",
    "        target_sinkhorn_threshold=target_sinkhorn_threshold,\n",
    "        max_attempts=max_attempts,\n",
    "        top_k=top_k,\n",
    "        color_space=color_space,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=steps,\n",
    "        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "        attn_ip_scale=attn_ip_scale,\n",
    "        text_token_scale=text_token_scale,\n",
    "        ip_token_scale=ip_token_scale,\n",
    "        ip_uncond_scale=ip_uncond_scale,\n",
    "        zero_ip_in_uncond=zero_ip_in_uncond,\n",
    "        verbose=True\n",
    "    )\n",
    "    sinkhorn_time = time.time() - start_time\n",
    "    sinkhorn_generated_image = sinkhorn_images[0]\n",
    "    \n",
    "    # Calculate edge metrics for sinkhorn\n",
    "    f1_e, iou_e, ssim_e = _edge_scores(_canny_bool(sinkhorn_generated_image), control_bool)\n",
    "    \n",
    "    sinkhorn_metrics = {\n",
    "        'generation_time': f\"{sinkhorn_time:.2f}\",\n",
    "        'sinkhorn': sinkhorn_score,\n",
    "        'cosine': sinkhorn_cosine,\n",
    "        'iou': iou_e,\n",
    "        'attempts': attempts\n",
    "    }\n",
    "    \n",
    "    print(f\"Sinkhorn generation: {sinkhorn_time:.2f}s, Sinkhorn: {sinkhorn_score:.4f}, Cosine: {sinkhorn_cosine:.4f}, IoU: {iou_e:.4f}, Attempts: {attempts}\")\n",
    "    \n",
    "    # 3. Create comprehensive visualization\n",
    "    print(\"\\nCreating comprehensive visualization...\")\n",
    "    \n",
    "    visualization = visualize_generation_comparison(\n",
    "        color_source_image=pil_img,\n",
    "        edge_map_image=control_image,\n",
    "        style_generated_image=style_generated_image,\n",
    "        sinkhorn_generated_image=sinkhorn_generated_image,\n",
    "        color_histogram=color_histogram,\n",
    "        color_space=color_space,\n",
    "        style_metrics=style_metrics,\n",
    "        sinkhorn_metrics=sinkhorn_metrics,\n",
    "        grid_size=256,\n",
    "        font_size=16\n",
    "    )\n",
    "    \n",
    "    print(f\"Visualization created: {visualization.size}\")\n",
    "    print(\"Final comparison:\")\n",
    "    print(f\"   Style: Sinkhorn={style_sinkhorn:.4f}, Cosine={style_cosine:.4f}, IoU={iou_s:.4f}\")\n",
    "    print(f\"   Sinkhorn:   Sinkhorn={sinkhorn_score:.4f}, Cosine={sinkhorn_cosine:.4f}, IoU={iou_e:.4f}\")\n",
    "    \n",
    "    from IPython.display import display\n",
    "    display(visualization)\n",
    "    \n",
    "    return visualization, style_metrics, sinkhorn_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7afb969",
   "metadata": {},
   "source": [
    "# 1st Generation Scenario\n",
    "\n",
    "We generate the images using a fixed prompt and settings for comparability, same layout image, and three different versions of the advertisement with 3 different color palettes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6158814",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_30_1000, style_metrics_30_1000, sinkhorn_metrics_30_1000 = generate_comparison_snippet(\n",
    "    color_index=1000,\n",
    "    layout_index=30,\n",
    "    prompt=\"a car, advertisement style, professional photography\",\n",
    "    guidance_scale=7,\n",
    "    steps=30,\n",
    "    controlnet_conditioning_scale=0.5,\n",
    "    num_samples=1,\n",
    "    attn_ip_scale=0.6,\n",
    "    text_token_scale=1.5,\n",
    "    ip_token_scale=0.4,\n",
    "    ip_uncond_scale=0.0,\n",
    "    zero_ip_in_uncond=False,\n",
    "    target_sinkhorn_threshold=0.02,\n",
    "    max_attempts=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_30_1001, style_metrics_30_1001, sinkhorn_metrics_30_1001 = generate_comparison_snippet(\n",
    "    color_index=1001,\n",
    "    layout_index=30,\n",
    "    prompt=\"a car, advertisement style, professional photography\",\n",
    "    guidance_scale=7,\n",
    "    steps=30,\n",
    "    controlnet_conditioning_scale=0.5,\n",
    "    num_samples=1,\n",
    "    attn_ip_scale=0.6,\n",
    "    text_token_scale=1.5,\n",
    "    ip_token_scale=0.4,\n",
    "    ip_uncond_scale=0.0,\n",
    "    zero_ip_in_uncond=False,\n",
    "    target_sinkhorn_threshold=0.02,\n",
    "    max_attempts=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_30_1008, style_metrics_30_1008, sinkhorn_metrics_30_1008 = generate_comparison_snippet(\n",
    "    color_index=1008,\n",
    "    layout_index=30,\n",
    "    prompt=\"a car, advertisement style, professional photography\",\n",
    "    guidance_scale=7,\n",
    "    steps=30,\n",
    "    controlnet_conditioning_scale=0.5,\n",
    "    num_samples=1,\n",
    "    attn_ip_scale=0.6,\n",
    "    text_token_scale=1.5,\n",
    "    ip_token_scale=0.4,\n",
    "    ip_uncond_scale=0.0,\n",
    "    zero_ip_in_uncond=False,\n",
    "    target_sinkhorn_threshold=0.02,\n",
    "    max_attempts=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bc058",
   "metadata": {},
   "source": [
    "# 2nd Generation Scenario\n",
    "\n",
    "Similar to 1st generation, but different layout image (footer banner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf51ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_71_1000, style_metrics_71_1000, sinkhorn_metrics_71_1000 = generate_comparison_snippet(\n",
    "    color_index=1000,\n",
    "    layout_index=71,\n",
    "    prompt=\"a car, advertisement style, professional photography\",\n",
    "    guidance_scale=7,\n",
    "    steps=30,\n",
    "    controlnet_conditioning_scale=0.5,\n",
    "    num_samples=1,\n",
    "    attn_ip_scale=0.6,\n",
    "    text_token_scale=1.5,\n",
    "    ip_token_scale=0.4,\n",
    "    ip_uncond_scale=0.0,\n",
    "    zero_ip_in_uncond=False,\n",
    "    target_sinkhorn_threshold=0.02,\n",
    "    max_attempts=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_71_1001, style_metrics_71_1001, sinkhorn_metrics_71_1001 = generate_comparison_snippet(\n",
    "    color_index=1001,\n",
    "    layout_index=71,\n",
    "    prompt=\"a car, advertisement style, professional photography\",\n",
    "    guidance_scale=7,\n",
    "    steps=30,\n",
    "    controlnet_conditioning_scale=0.5,\n",
    "    num_samples=1,\n",
    "    attn_ip_scale=0.6,\n",
    "    text_token_scale=1.5,\n",
    "    ip_token_scale=0.4,\n",
    "    ip_uncond_scale=0.0,\n",
    "    zero_ip_in_uncond=False,\n",
    "    target_sinkhorn_threshold=0.02,\n",
    "    max_attempts=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c66114",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_71_1008, style_metrics_71_1008, sinkhorn_metrics_71_1008 = generate_comparison_snippet(\n",
    "    color_index=1008,\n",
    "    layout_index=71,\n",
    "    prompt=\"a car, advertisement style, professional photography\",\n",
    "    guidance_scale=7,\n",
    "    steps=30,\n",
    "    controlnet_conditioning_scale=0.5,\n",
    "    num_samples=1,\n",
    "    attn_ip_scale=0.6,\n",
    "    text_token_scale=1.5,\n",
    "    ip_token_scale=0.4,\n",
    "    ip_uncond_scale=0.0,\n",
    "    zero_ip_in_uncond=False,\n",
    "    target_sinkhorn_threshold=0.02,\n",
    "    max_attempts=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d871de2",
   "metadata": {},
   "source": [
    "# 3rd Generation Scenario\n",
    "Features 1 layout and 2 color palettes, a prompt that doesn't describe the layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_400_1000, style_metrics_400_1000, sinkhorn_metrics_400_1000 = generate_comparison_snippet(\n",
    "    color_index=1000,\n",
    "    layout_index=400,\n",
    "    prompt=\"a boat, advertisement style, professional photography\",\n",
    "    guidance_scale=7,\n",
    "    steps=50,\n",
    "    controlnet_conditioning_scale=0.4,\n",
    "    num_samples=1,\n",
    "    attn_ip_scale=0.6,\n",
    "    text_token_scale=2.5,\n",
    "    ip_token_scale=0.7,\n",
    "    ip_uncond_scale=0.0,\n",
    "    zero_ip_in_uncond=False,\n",
    "    target_sinkhorn_threshold=0.02,\n",
    "    max_attempts=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2445d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_400_1008, style_metrics_400_1008, sinkhorn_metrics_400_1008 = generate_comparison_snippet(\n",
    "    color_index=1008,\n",
    "    layout_index=400,\n",
    "    prompt=\"a boat, advertisement style, professional photography\",\n",
    "    guidance_scale=7,\n",
    "    steps=50,\n",
    "    controlnet_conditioning_scale=0.4,\n",
    "    num_samples=1,\n",
    "    attn_ip_scale=0.6,\n",
    "    text_token_scale=3.5,\n",
    "    ip_token_scale=0.9,\n",
    "    ip_uncond_scale=0.0,\n",
    "    zero_ip_in_uncond=False,\n",
    "    target_sinkhorn_threshold=0.02,\n",
    "    max_attempts=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e71e45a",
   "metadata": {},
   "source": [
    "# Quantitative Evaluation Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tidy evaluation DataFrame for N prompt/layout pairs\n",
    "import time, numpy as np, pandas as pd, torch, gc\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from degis.inference import generate_by_style, generate_by_color_sinkhorn_constrained\n",
    "from degis.inference.core_generation import get_color_embedding\n",
    "from degis.shared.utils.image_utils import create_control_edge_pil\n",
    "from degis.inference.generation_functions import (\n",
    "    compute_histogram_for_color_space,\n",
    "    calculate_sinkhorn_distance_topk,\n",
    "    calculate_cosine_similarity,\n",
    ")\n",
    "\n",
    "# ---- edge helpers (F1 / IoU / SSIM) ----\n",
    "def _canny_bool(pil_img, sigma=1.0):\n",
    "    import numpy as np\n",
    "    try:\n",
    "        from skimage.color import rgb2gray\n",
    "        from skimage.feature import canny\n",
    "        g = rgb2gray(np.asarray(pil_img).astype(np.float32) / 255.0)\n",
    "        e = canny(g, sigma=sigma)\n",
    "        return e.astype(np.uint8)\n",
    "    except Exception:\n",
    "        from PIL import ImageFilter\n",
    "        e = pil_img.convert(\"L\").filter(ImageFilter.FIND_EDGES)\n",
    "        return (np.asarray(e) > 32).astype(np.uint8)\n",
    "\n",
    "def _edge_scores(pred_bool, ref_bool):\n",
    "    import numpy as np\n",
    "    pred = pred_bool.astype(bool); ref = ref_bool.astype(bool)\n",
    "    tp = np.logical_and(pred, ref).sum()\n",
    "    fp = np.logical_and(pred, ~ref).sum()\n",
    "    fn = np.logical_and(~pred, ref).sum()\n",
    "    prec = tp / (tp + fp + 1e-9)\n",
    "    rec  = tp / (tp + fn + 1e-9)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    iou  = tp / (tp + fp + fn + 1e-9)\n",
    "    try:\n",
    "        from skimage.metrics import structural_similarity as ssim\n",
    "        ssimv = ssim(pred_bool.astype(np.float32), ref_bool.astype(np.float32))\n",
    "    except Exception:\n",
    "        ssimv = np.nan\n",
    "    return f1, iou, ssimv\n",
    "\n",
    "toPIL = transforms.ToPILImage()\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_eval_pairs(\n",
    "    pairs,\n",
    "    prompt=\"a car, advertisement style, professional photography\",\n",
    "    guidance_scale=7,\n",
    "    steps=30,\n",
    "    controlnet_conditioning_scale=0.5,\n",
    "    attn_ip_scale=0.6,\n",
    "    text_token_scale=1.5,\n",
    "    ip_token_scale=0.4,\n",
    "    ip_uncond_scale=0.0,\n",
    "    zero_ip_in_uncond=False,\n",
    "    # palette metrics\n",
    "    color_space=\"lab\",\n",
    "    bins=8,\n",
    "    # Sinkhorn-constrained outer loop\n",
    "    target_sinkhorn_threshold=0.02,\n",
    "    max_attempts=20,\n",
    "    top_k=20,\n",
    "    verbose=False,\n",
    "):\n",
    "    rows = []\n",
    "    dev = generator.device\n",
    "\n",
    "    for color_index, layout_index in pairs:\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        # --- source PIL for style baseline + target histogram\n",
    "        img_t, _ = color_dataset[color_index]         # uses the UnifiedImageDataset\n",
    "        pil_img  = toPIL(img_t)\n",
    "\n",
    "        target_hist = compute_histogram_for_color_space(pil_img, color_space, bins=bins)\n",
    "\n",
    "        # Compute CLIP embedding\n",
    "        from degis.shared.clip_vit_bigg14 import compute_clip_embedding_xl\n",
    "        z_clip = compute_clip_embedding_xl(pil_img).to(device).unsqueeze(0)\n",
    "        color_embedding = get_color_embedding(color_head, z_clip)\n",
    "\n",
    "        # --- control (edge) image + boolean mask for metrics\n",
    "        control_img  = create_control_edge_pil(edge_maps[layout_index], size=512)\n",
    "        control_bool = (np.asarray(control_img.convert(\"L\")) > 0).astype(np.uint8)\n",
    "\n",
    "        # ============ 1) STYLE baseline ============\n",
    "        t0 = time.time()\n",
    "        style_images = generate_by_style(\n",
    "            generator=generator,\n",
    "            pil_image=pil_img,                          # Pass PIL (not clip_image_embeds)\n",
    "            control_image=control_img,\n",
    "            prompt=prompt,\n",
    "            num_samples=1,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=steps,\n",
    "            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "            attn_ip_scale=attn_ip_scale,\n",
    "            text_token_scale=text_token_scale,\n",
    "            ip_token_scale=ip_token_scale,\n",
    "            ip_uncond_scale=ip_uncond_scale,\n",
    "            zero_ip_in_uncond=zero_ip_in_uncond,\n",
    "        )\n",
    "        style_time = time.time() - t0\n",
    "        style_img  = style_images[0]\n",
    "\n",
    "        style_hist = compute_histogram_for_color_space(style_img, color_space, bins=bins)\n",
    "        eval_top_k = min(top_k, len(target_hist)) if top_k is not None else len(target_hist)\n",
    "        style_sinkhorn  = calculate_sinkhorn_distance_topk(target_hist, style_hist, top_k=eval_top_k, blur=0.01)\n",
    "\n",
    "        style_cos  = calculate_cosine_similarity(prompt, style_img)\n",
    "        f1_s, iou_s, ssim_s = _edge_scores(_canny_bool(style_img), control_bool)\n",
    "\n",
    "        # ============ 2) Sinkhorn-constrained (palette-tokens) ============\n",
    "        t1 = time.time()\n",
    "        sinkhorn_images, sinkhorn_score, sinkhorn_cos, attempts = generate_by_color_sinkhorn_constrained(\n",
    "            generator=generator,\n",
    "            color_embedding=color_embedding,\n",
    "            control_image=control_img,\n",
    "            original_histogram=target_hist,\n",
    "            prompt=prompt,\n",
    "            target_sinkhorn_threshold=target_sinkhorn_threshold,\n",
    "            max_attempts=max_attempts,\n",
    "            top_k=top_k,\n",
    "            color_space=color_space,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=steps,\n",
    "            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "            attn_ip_scale=attn_ip_scale,\n",
    "            text_token_scale=text_token_scale,\n",
    "            ip_token_scale=ip_token_scale,\n",
    "            ip_uncond_scale=ip_uncond_scale,\n",
    "            zero_ip_in_uncond=zero_ip_in_uncond,\n",
    "            verbose=False,\n",
    "        )\n",
    "        sinkhorn_time = time.time() - t1\n",
    "        sinkhorn_img  = sinkhorn_images[0]\n",
    "        f1_e, iou_e, ssim_e = _edge_scores(_canny_bool(sinkhorn_img), control_bool)\n",
    "\n",
    "        rows.append(dict(\n",
    "            color_index=int(color_index),\n",
    "            layout_index=int(layout_index),\n",
    "            prompt=prompt,\n",
    "            style_sinkhorn=float(style_sinkhorn), style_cos=float(style_cos),\n",
    "            sinkhorn_sinkhorn=float(sinkhorn_score),   sinkhorn_cos=float(sinkhorn_cos),\n",
    "            style_f1=float(f1_s), style_iou=float(iou_s), style_ssim=float(ssim_s),\n",
    "            sinkhorn_f1=float(f1_e),   sinkhorn_iou=float(iou_e),   sinkhorn_ssim=float(ssim_e),\n",
    "            attempts=int(attempts),\n",
    "            style_time=float(style_time), sinkhorn_time=float(sinkhorn_time),\n",
    "        ))\n",
    "        if verbose:\n",
    "            print(f\"[{color_index},{layout_index}] ΔSinkhorn={style_sinkhorn-sinkhorn_score:+.5f} | attempts={attempts}\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    display(df.head())\n",
    "    print(f\"Collected {len(df)} rows.\")\n",
    "    return df\n",
    "\n",
    "# Example usage - use specific IDs for controlled evaluation\n",
    "pairs = [(int(i), int(j)) for i in range(1000, 1020)  # color_index: 1000-1019 (20 images)\n",
    "         for j in [30, 71, 400, 507]]  # layout_index: 4 specific layouts\n",
    "\n",
    "print(\"Testing these color_index, layout_index pairs:\")\n",
    "for i, (c_idx, l_idx) in enumerate(pairs):\n",
    "    print(f\"  {i+1:2d}: color_index={c_idx:7d}, layout_index={l_idx:4d}\")\n",
    "print(f\"Total pairs: {len(pairs)}\")  # Should be 20 * 4 = 80 pairs\n",
    "\n",
    "df_eval = run_eval_pairs(pairs, target_sinkhorn_threshold=0.05, max_attempts=20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24184a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval-1 - Pass@{0.020, 0.025, 0.030}\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "def pass_at_t(df, taus=(0.020, 0.025, 0.030), col=\"sinkhorn_sinkhorn\"):\n",
    "    out = []\n",
    "    for t in taus:\n",
    "        passed = (df[col].values <= t).mean()\n",
    "        out.append({\"tau\": t, \"pass_rate\": float(passed)})\n",
    "    res = pd.DataFrame(out)\n",
    "    display(res)\n",
    "    return res\n",
    "\n",
    "print(\"Palette-token run (Sinkhorn-constrained):\")\n",
    "pass_at_t(df_eval, taus=(0.020, 0.025, 0.030), col=\"sinkhorn_sinkhorn\")\n",
    "\n",
    "print(\"\\nSTYLE baseline (for context):\")\n",
    "pass_at_t(df_eval, taus=(0.020, 0.025, 0.030), col=\"style_sinkhorn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval-2 - paired improvement over STYLE baseline\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "df_delta = df_eval.assign(delta_sinkhorn = df_eval[\"style_sinkhorn\"] - df_eval[\"sinkhorn_sinkhorn\"])\n",
    "print(f\"ΔSinkhorn (style − palette): mean={df_delta['delta_sinkhorn'].mean():.5f}, \"\n",
    "      f\"median={df_delta['delta_sinkhorn'].median():.5f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df_delta[\"delta_sinkhorn\"], bins=20)\n",
    "plt.xlabel(\"ΔSinkhorn (positive = palette better)\"); plt.ylabel(\"count\"); plt.title(\"ΔSinkhorn vs STYLE baseline\")\n",
    "plt.show()\n",
    "\n",
    "df_delta.sort_values(\"delta_sinkhorn\", ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9bbefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval-3 - layout fidelity (edge F1 / IoU / SSIM)\n",
    "\n",
    "def _brief_stats(x): \n",
    "    import numpy as np\n",
    "    return f\"mean={np.nanmean(x):.3f}  std={np.nanstd(x):.3f}\"\n",
    "\n",
    "print(\"STYLE   - F1:\", _brief_stats(df_eval[\"style_f1\"].values),\n",
    "      \"| IoU:\", _brief_stats(df_eval[\"style_iou\"].values),\n",
    "      \"| SSIM:\", _brief_stats(df_eval[\"style_ssim\"].values))\n",
    "\n",
    "print(\"PALETTE - F1:\", _brief_stats(df_eval[\"sinkhorn_f1\"].values),\n",
    "      \"| IoU:\", _brief_stats(df_eval[\"sinkhorn_iou\"].values),\n",
    "      \"| SSIM:\", _brief_stats(df_eval[\"sinkhorn_ssim\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b76f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval-4 - text–image cosine (higher = better)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "m_style  = df_eval[\"style_cos\"].mean()\n",
    "m_sinkhorn    = df_eval[\"sinkhorn_cos\"].mean()\n",
    "delta    = (df_eval[\"sinkhorn_cos\"] - df_eval[\"style_cos\"]).mean()\n",
    "\n",
    "print(f\"CLIP cosine - STYLE mean={m_style:.4f} | PALETTE mean={m_sinkhorn:.4f} | Δ (palette−style)={delta:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval-5 - sweep ip_token_scale ∈ {0.4, 0.7, 1.0} on a small subset\n",
    "\n",
    "scales = [0.4, 0.7, 1.0]\n",
    "subset_pairs = df_eval[[\"color_index\",\"layout_index\"]].head(10).itertuples(index=False, name=None)\n",
    "\n",
    "rows = []\n",
    "for s in scales:\n",
    "    df_s = run_eval_pairs(list(subset_pairs),\n",
    "                          ip_token_scale=s,\n",
    "                          target_sinkhorn_threshold=0.025,\n",
    "                          max_attempts=20,\n",
    "                          verbose=False)\n",
    "    rows.append({\"ip_token_scale\": s,\n",
    "                 \"sinkhorn_mean\": df_s[\"sinkhorn_sinkhorn\"].mean(),\n",
    "                 \"sinkhorn_median\": df_s[\"sinkhorn_sinkhorn\"].median(),\n",
    "                 \"pass@0.025\": (df_s[\"sinkhorn_sinkhorn\"] <= 0.025).mean()})\n",
    "    # reset iterator\n",
    "    subset_pairs = df_eval[[\"color_index\",\"layout_index\"]].head(10).itertuples(index=False, name=None)\n",
    "\n",
    "df_sweep = pd.DataFrame(rows)\n",
    "display(df_sweep)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(df_sweep[\"ip_token_scale\"], df_sweep[\"sinkhorn_mean\"], marker=\"o\")\n",
    "plt.xlabel(\"ip_token_scale\"); plt.ylabel(\"Sinkhorn (mean)\"); plt.title(\"Sensitivity of palette strength\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5421797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval-6 - attempts statistics and success rate within budget\n",
    "\n",
    "tau = 0.025\n",
    "budget = 20\n",
    "\n",
    "succ = (df_eval[\"sinkhorn_sinkhorn\"] <= tau).mean()\n",
    "att_succ = df_eval.loc[df_eval[\"sinkhorn_sinkhorn\"] <= tau, \"attempts\"]\n",
    "print(f\"Success@τ={tau:.3f} within {budget} attempts: {succ*100:.1f}%\")\n",
    "if len(att_succ):\n",
    "    print(f\"Attempts for successful runs - mean={att_succ.mean():.2f}, median={att_succ.median():.1f}, max={att_succ.max()}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(df_eval[\"attempts\"], bins=range(1, budget+2))\n",
    "plt.xlabel(\"Attempts\"); plt.ylabel(\"count\"); plt.title(\"Outer-loop attempts distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e537d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval-7 - trainable params added (lightweight claim)\n",
    "\n",
    "import torch, json, glob, os\n",
    "\n",
    "color_params = sum(p.numel() for p in color_head.parameters() if p.requires_grad)\n",
    "rest_params  = None\n",
    "\n",
    "# try to read rest from any best_summary.json (if present)\n",
    "try:\n",
    "    any_summary = sorted(glob.glob(\"evaluation_runs/**/best_summary.json\", recursive=True))[0]\n",
    "    with open(any_summary, \"r\") as f:\n",
    "        js = json.load(f)\n",
    "    rest_params = js.get(\"param_count_rest\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if rest_params is None:\n",
    "    print(f\"Trainable params - ColorHead: {color_params:,} (RestHead: n/a at inference)\")\n",
    "else:\n",
    "    print(f\"Trainable params - ColorHead: {color_params:,} | RestHead: {int(rest_params):,} | Total: {color_params+int(rest_params):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb660ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helpers (edges, metrics) ---\n",
    "import numpy as np, time, torch\n",
    "from PIL import Image\n",
    "try:\n",
    "    from skimage.feature import canny as sk_canny\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    _HAS_SKIMAGE = True\n",
    "except Exception:\n",
    "    _HAS_SKIMAGE = False\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "def _edge_from_pil(img: Image.Image, sigma: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"0..1 float edge map from PIL image; uses skimage if available, else a simple Sobel fallback.\"\"\"\n",
    "    g = np.asarray(img.convert(\"L\"), dtype=np.float32) / 255.0\n",
    "    if _HAS_SKIMAGE:\n",
    "        e = sk_canny(g, sigma=sigma).astype(np.float32)\n",
    "    else:\n",
    "        gx = np.gradient(g, axis=1); gy = np.gradient(g, axis=0)\n",
    "        mag = np.hypot(gx, gy)\n",
    "        thr = np.quantile(mag, 0.90)\n",
    "        e = (mag >= thr).astype(np.float32)\n",
    "    return e\n",
    "\n",
    "def _bin(x: ArrayLike, thr: float = 0.5) -> np.ndarray:\n",
    "    return (np.asarray(x, dtype=np.float32) >= thr).astype(np.uint8)\n",
    "\n",
    "def f1_iou(pred: ArrayLike, targ: ArrayLike, thr: float = 0.5):\n",
    "    \"\"\"F1 and IoU on binarised maps.\"\"\"\n",
    "    p = _bin(pred, thr); t = _bin(targ, thr)\n",
    "    tp = np.sum((p == 1) & (t == 1))\n",
    "    fp = np.sum((p == 1) & (t == 0))\n",
    "    fn = np.sum((p == 0) & (t == 1))\n",
    "    denom_f1 = (2*tp + fp + fn)\n",
    "    f1 = (2*tp / denom_f1) if denom_f1 else 1.0\n",
    "    denom_iou = (tp + fp + fn)\n",
    "    iou = (tp / denom_iou) if denom_iou else 1.0\n",
    "    return float(f1), float(iou)\n",
    "\n",
    "# --- evaluation buffer ---\n",
    "records = []  # each item is a dict (see below)\n",
    "\n",
    "# --- one-shot case evaluator (returns a dict; does NOT mutate global state) ---\n",
    "def evaluate_case(\n",
    "    *,                                   # keyword-only for clarity\n",
    "    color_img: Image.Image,             # style reference image (PIL)\n",
    "    control_edge_ref: Image.Image,       # reference edge map (PIL, 0..255)\n",
    "    color_head,                          # trained color head (nn.Module)\n",
    "    generator,                           # IP-Adapter pipeline wrapper\n",
    "    prompt: str,\n",
    "    z_clip_row: np.ndarray,              # 1D numpy (D,)\n",
    "    color_space: str = \"lab\",\n",
    "    bins: int = 80,                      # for on-the-fly histogram + Sinkhorn\n",
    "    # gauges\n",
    "    attn_ip_scale: float = 0.7,\n",
    "    text_token_scale: float = 1.2,\n",
    "    ip_token_scale: float = 0.6,\n",
    "    ip_uncond_scale: float = 0.0,\n",
    "    zero_ip_in_uncond: bool = True,\n",
    "    # sampling\n",
    "    guidance_scale: float = 8.0,\n",
    "    steps: int = 60,\n",
    "    controlnet_conditioning_scale: float = 0.7,\n",
    "    # outer loop (only used in palette path helper if you set target)\n",
    "    target_sinkhorn_threshold: float | None = None,\n",
    "    max_attempts: int = 20,\n",
    "    top_k: int = 20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs (A) style baseline and (B) palette-tokens variant.\n",
    "    Returns a single dict with Sinkhorn/CLIP/edge metrics & timings for both paths.\n",
    "    \"\"\"\n",
    "    from degis.inference import generate_by_style, generate_by_color_sinkhorn_constrained\n",
    "    from degis.shared.utils.image_utils import create_control_edge_pil\n",
    "    from degis.inference.generation_functions import (\n",
    "        compute_histogram_for_color_space, calculate_sinkhorn_distance_topk, calculate_cosine_similarity,\n",
    "        get_color_embedding\n",
    "    )\n",
    "\n",
    "    # prep common things\n",
    "    t0 = time.time()\n",
    "    z_clip = torch.tensor(z_clip_row, dtype=torch.float32, device=\"cuda\" if torch.cuda.is_available() else \"cpu\").unsqueeze(0)\n",
    "    hist_color = compute_histogram_for_color_space(color_img, color_space=color_space, bins=bins)\n",
    "    # ref edges (0..1 float)\n",
    "    edge_ref = np.asarray(control_edge_ref.convert(\"L\"), dtype=np.float32) / 255.0\n",
    "\n",
    "    # --- (A) style baseline ---\n",
    "    tA = time.time()\n",
    "    style_imgs = generate_by_style(\n",
    "        generator=generator,\n",
    "        pil_image=color_img,\n",
    "        control_image=control_edge_ref,\n",
    "        prompt=prompt,\n",
    "        num_samples=1,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=steps,\n",
    "        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "        attn_ip_scale=attn_ip_scale,\n",
    "        text_token_scale=text_token_scale,\n",
    "        ip_token_scale=None,          # stock behaviour (style ref)\n",
    "        ip_uncond_scale=None,\n",
    "        zero_ip_in_uncond=False,\n",
    "    )\n",
    "    style_img = style_imgs[0]\n",
    "    tA = time.time() - tA\n",
    "\n",
    "    # metrics for style\n",
    "    style_hist = compute_histogram_for_color_space(style_img, color_space=color_space, bins=bins)\n",
    "    style_sinkhorn  = calculate_sinkhorn_distance_topk(hist_color, style_hist, top_k=top_k)\n",
    "    style_cos  = calculate_cosine_similarity(prompt, style_img)\n",
    "    style_edge = _edge_from_pil(style_img)\n",
    "    style_f1, style_iou = f1_iou(style_edge, edge_ref, thr=0.5)\n",
    "    style_ssim = float(ssim(edge_ref, style_edge)) if _HAS_SKIMAGE else np.nan\n",
    "\n",
    "    # --- (B) palette-tokens path (with outer loop disabled by default) ---\n",
    "    tB = time.time()\n",
    "    color_emb = get_color_embedding(color_head, z_clip)  # [B, D]\n",
    "    sinkhorn_imgs, sinkhorn_score, sinkhorn_cos, attempts = generate_by_color_sinkhorn_constrained(\n",
    "        generator=generator,\n",
    "        color_embedding=color_emb,\n",
    "        control_image=control_edge_ref,\n",
    "        original_histogram=hist_color,\n",
    "        prompt=prompt,\n",
    "        target_sinkhorn_threshold=target_sinkhorn_threshold,\n",
    "        max_attempts=max_attempts,\n",
    "        top_k=top_k,\n",
    "        color_space=color_space,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=steps,\n",
    "        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "        attn_ip_scale=attn_ip_scale,\n",
    "        text_token_scale=text_token_scale,\n",
    "        ip_token_scale=ip_token_scale,\n",
    "        ip_uncond_scale=ip_uncond_scale,\n",
    "        zero_ip_in_uncond=zero_ip_in_uncond,\n",
    "        verbose=False\n",
    "    )\n",
    "    sinkhorn_img = sinkhorn_imgs[0]\n",
    "    tB = time.time() - tB\n",
    "\n",
    "    # metrics for palette path\n",
    "    gen_hist  = compute_histogram_for_color_space(sinkhorn_img, color_space=color_space, bins=bins)\n",
    "    gen_sinkhorn   = calculate_sinkhorn_distance_topk(hist_color, gen_hist, top_k=top_k)\n",
    "    gen_cos   = calculate_cosine_similarity(prompt, sinkhorn_img)\n",
    "    gen_edge  = _edge_from_pil(sinkhorn_img)\n",
    "    gen_f1, gen_iou = f1_iou(gen_edge, edge_ref, thr=0.5)\n",
    "    gen_ssim = float(ssim(edge_ref, gen_edge)) if _HAS_SKIMAGE else np.nan\n",
    "\n",
    "    return {\n",
    "        # reference\n",
    "        \"edge_ref\": edge_ref,\n",
    "        # style path\n",
    "        \"style_img\": style_img, \"style_time\": tA, \"sinkhorn_style\": float(style_sinkhorn), \"cos_style\": float(style_cos),\n",
    "        \"edge_style\": style_edge, \"f1_style\": style_f1, \"iou_style\": style_iou, \"ssim_style\": style_ssim,\n",
    "        # palette path\n",
    "        \"sinkhorn_img\": sinkhorn_img, \"sinkhorn_time\": tB, \"sinkhorn_palette\": float(gen_sinkhorn), \"cos_palette\": float(gen_cos),\n",
    "        \"edge_sinkhorn\": gen_edge, \"f1_sinkhorn\": gen_f1, \"iou_sinkhorn\": gen_iou, \"ssim_sinkhorn\": gen_ssim,\n",
    "        \"attempts\": int(attempts),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds to report\n",
    "taus = [0.020, 0.025, 0.030]\n",
    "\n",
    "sinkhorn_vals_palette = np.array([r[\"sinkhorn_palette\"] for r in records], dtype=float)\n",
    "sinkhorn_vals_style   = np.array([r[\"sinkhorn_style\"]   for r in records], dtype=float)\n",
    "\n",
    "def _pass_at_tau(arr, tau): return float(np.mean(arr <= tau))\n",
    "\n",
    "print(\"Pass@τ (palette-tokens)  |  (style baseline)\")\n",
    "for t in taus:\n",
    "    print(f\"  τ={t:.3f}:  { _pass_at_tau(sinkhorn_vals_palette,t):.3f}   |   { _pass_at_tau(sinkhorn_vals_style,t):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf062bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = sinkhorn_vals_style - sinkhorn_vals_palette  # positive = Sinkhorn-constrained method improves\n",
    "print(f\"ΔSinkhorn (style − palette): mean={delta.mean():.5f}, median={np.median(delta):.5f}\")\n",
    "hist, edges = np.histogram(delta, bins=7)\n",
    "print(\"hist ΔSinkhorn:\", hist.tolist())\n",
    "print(\"bins:\", [round(x,5) for x in edges.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_s = np.array([r[\"f1_style\"] for r in records], dtype=float)\n",
    "f1_p = np.array([r[\"f1_sinkhorn\"]   for r in records], dtype=float)\n",
    "iou_s = np.array([r[\"iou_style\"] for r in records], dtype=float)\n",
    "iou_p = np.array([r[\"iou_sinkhorn\"]   for r in records], dtype=float)\n",
    "ssim_s = np.array([r[\"ssim_style\"] for r in records], dtype=float)\n",
    "ssim_p = np.array([r[\"ssim_sinkhorn\"]   for r in records], dtype=float)\n",
    "\n",
    "def _m(m): return f\"{np.nanmean(m):.3f}±{np.nanstd(m):.3f}\"\n",
    "\n",
    "print(\"Edge adherence  (mean±sd)\")\n",
    "print(f\"  F1:   style={_m(f1_s)}   palette={_m(f1_p)}\")\n",
    "print(f\"  IoU:  style={_m(iou_s)}  palette={_m(iou_p)}\")\n",
    "print(f\"  SSIM: style={_m(ssim_s)} palette={_m(ssim_p)}  (nan if skimage not present)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_s = np.array([r[\"cos_style\"] for r in records], dtype=float)\n",
    "cos_p = np.array([r[\"cos_palette\"] for r in records], dtype=float)\n",
    "print(f\"CLIP cosine - style:   mean={cos_s.mean():.4f}\")\n",
    "print(f\"CLIP cosine - palette: mean={cos_p.mean():.4f}\")\n",
    "print(f\"Δ (palette − style):   mean={(cos_p - cos_s).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d06914",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [0.4, 0.7, 1.0]\n",
    "subset_idx = list(range(min(20, len(records))))\n",
    "table = {s: [] for s in scales}\n",
    "\n",
    "# We reuse the same inputs as the original record, but re-run only palette path:\n",
    "for s in scales:\n",
    "    vals = []\n",
    "    for i in subset_idx:\n",
    "        r = records[i]\n",
    "        case = evaluate_case(\n",
    "            color_img=r[\"color_img\"],                 # <-- store these when creating records\n",
    "            control_edge_ref=r[\"control_edge_ref\"],\n",
    "            color_head=color_head,\n",
    "            generator=generator,\n",
    "            prompt=r[\"prompt\"],\n",
    "            z_clip_row=r[\"z_clip_row\"],\n",
    "            ip_token_scale=s,\n",
    "            text_token_scale=1.2,\n",
    "            attn_ip_scale=0.7,\n",
    "            zero_ip_in_uncond=True,\n",
    "            ip_uncond_scale=0.0,\n",
    "            guidance_scale=8.0, steps=60,\n",
    "            controlnet_conditioning_scale=0.7,\n",
    "        )\n",
    "        vals.append(case[\"sinkhorn_palette\"])\n",
    "    table[s] = vals\n",
    "\n",
    "print(\"Sinkhorn vs ip_token_scale (lower is better)\")\n",
    "for s in scales:\n",
    "    arr = np.array(table[s], dtype=float)\n",
    "    print(f\"  {s:.1f}: mean={arr.mean():.4f}, median={np.median(arr):.4f}, n={len(arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempts = np.array([r[\"attempts\"] for r in records], dtype=float)\n",
    "sinkhorn_p     = np.array([r[\"sinkhorn_palette\"] for r in records], dtype=float)\n",
    "\n",
    "# success rates within a budget (e.g., 20 attempts) for thresholds:\n",
    "taus = [0.020, 0.025, 0.030]\n",
    "budget = 20\n",
    "print(f\"Attempts stats: mean={attempts.mean():.2f}, median={np.median(attempts):.1f}\")\n",
    "for t in taus:\n",
    "    succ = (sinkhorn_p <= t) & (attempts <= budget)\n",
    "    print(f\"Success@{t:.3f} within {budget} attempts: {succ.mean():.3f}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb297ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eval.to_csv(\"evaluation_runs/evaluation_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b7620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556420b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d59c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "degis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
