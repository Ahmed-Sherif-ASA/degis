{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Notebook: SD\u20111.5 + ControlNet (Canny) + IP\u2011Adapter (Colour Head)\n",
        "\n",
        "This notebook reproduces the `evaluate.py` functionality in Jupyter form.\n",
        "\n",
        "**What it does**\n",
        "1. Loads your manifest, CLIP embeddings `[N,1024]`, and colour histograms `[N,K]` (RGB512 / Lab514 / HCL514).\n",
        "2. Builds SD\u20111.5 + ControlNet(Canny) and your forked IP\u2011Adapter that accepts a **custom colour embedding**.\n",
        "3. Runs the **2\u00d72 ablation** (Text / +Edges / +Colour / +Both) and the **scale grid** (IP scale \u00d7 CN scale).\n",
        "4. Computes metrics: **Colour\u2011EMD (Sinkhorn)**, **neutral\u2011bin errors** (Lab/HCL), **Edge\u2011F1**, **Edge\u2011SSIM**, **CLIP\u2011Score**, **latency**.\n",
        "5. Saves images, montages, and a per\u2011image **metrics CSV**.\n",
        "\n",
        "> **Note:** This expects your **forked `ip_adapter`** module to be importable and to support `embedding_type='custom'` so you can pass the Colour Head embedding directly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Prerequisites (install as needed)\n",
        "Uncomment and run the lines below if your environment is missing dependencies.\n",
        "\n",
        "```bash\n",
        "# pip install --upgrade pip\n",
        "# pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121  # adapt to your CUDA\n",
        "# pip install diffusers accelerate transformers safetensors\n",
        "# pip install open-clip-torch\n",
        "# pip install geomloss scikit-image opencv-python pandas matplotlib tqdm\n",
        "```\n",
        "\n",
        "Make sure your project folder (with the forked `ip_adapter` and your `models` package) is on `sys.path`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": true
        }
      },
      "source": [
        "import os, sys, time, math, json\n",
        "from types import SimpleNamespace\n",
        "from pathlib import Path\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageOps, ImageDraw\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import cv2\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# If your project repo is not in sys.path, add it here, e.g.:\n",
        "# sys.path.append('/data/thesis/repo')\n",
        "\n",
        "try:\n",
        "    from ip_adapter import IPAdapter  # your fork\n",
        "except Exception as e:\n",
        "    print(\"\u26a0\ufe0f Could not import ip_adapter. Add your repo to sys.path.\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    from models.color_heads import ColorHead\n",
        "except Exception:\n",
        "    # fallback to alternative path if your module layout differs\n",
        "    try:\n",
        "        from color_heads import ColorHead\n",
        "    except Exception as e:\n",
        "        print(\"\u26a0\ufe0f Could not import ColorHead. Adjust the import to your repo layout.\")\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    from geomloss import SamplesLoss\n",
        "    HAVE_GEOMLOSS = True\n",
        "except Exception:\n",
        "    HAVE_GEOMLOSS = False\n",
        "\n",
        "import open_clip\n",
        "from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Configuration\n",
        "Fill in your paths and knobs below. You can run multiple cells with different configs to generate into separate output folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cfg = SimpleNamespace(\n",
        "    # Data\n",
        "    manifest_csv = \"/data/thesis/laion_5m_manifest.csv\",  # must have file_path column (or local_path)\n",
        "    embeddings_npy = \"/data/degis/embeddings.npy\",        # [N,1024]\n",
        "    hists_npy = \"/data/degis/hcl514.npy\",                 # [N,514] for HCL / Lab; [N,512] for RGB\n",
        "    color_space = \"hcl514\",                                # one of: rgb512 | lab514 | hcl514\n",
        "    color_head_ckpt = \"/data/degis/best_color_head.pth\",  # trained head matching hist dim\n",
        "    prompts_csv = \"/data/degis/prompts_25.csv\",           # columns: idx,prompt\n",
        "\n",
        "    # Models\n",
        "    sd_id = \"runwayml/stable-diffusion-v1-5\",\n",
        "    controlnet_id = \"lllyasviel/control_v11p_sd15_canny\",\n",
        "    ip_ckpt = \"/data/thesis/models/ip-adapter_sd15.bin\",\n",
        "    image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\",\n",
        "    hf_cache = None,  # or a path\n",
        "\n",
        "    # Sweeps\n",
        "    ip_scales = [0.25, 0.5, 0.75, 1.0],\n",
        "    cn_scales = [0.8, 1.2, 1.6, 2.0],\n",
        "    cfg_scale = 7.5,\n",
        "    steps = 50,\n",
        "    seed = 123,\n",
        "\n",
        "    # Edges\n",
        "    canny_low = 100,\n",
        "    canny_high = 200,\n",
        "    edge_size = 512,\n",
        "\n",
        "    # Output\n",
        "    outdir = \"/data/degis/eval_hcl514_notebook\",\n",
        "    limit = 0,  # 0 => all rows in prompts_csv\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cfg, device"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Helpers: IO, histograms, metrics, montages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def ensure_dir(p):\n",
        "    Path(p).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_manifest(manifest_csv):\n",
        "    df = pd.read_csv(manifest_csv)\n",
        "    if \"file_path\" not in df.columns and \"local_path\" in df.columns:\n",
        "        df = df.rename(columns={\"local_path\": \"file_path\"})\n",
        "    assert \"file_path\" in df.columns, \"manifest must have a 'file_path' column\"\n",
        "    return df\n",
        "\n",
        "# --- histograms ---\n",
        "def rgb_hist(img_np, bins=8):\n",
        "    hist, _ = np.histogramdd(\n",
        "        img_np.reshape(-1, 3), bins=(bins,bins,bins), range=((0,256),(0,256),(0,256))\n",
        "    )\n",
        "    h = hist.flatten().astype(np.float32)\n",
        "    h /= (h.sum() + 1e-8)\n",
        "    return h\n",
        "\n",
        "def lab_hist(img_np, bins=8):\n",
        "    lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2Lab)\n",
        "    L,a,b = cv2.split(lab)\n",
        "    neutral_th = 5\n",
        "    is_black = (L < 25) & (np.abs(a-128)<neutral_th) & (np.abs(b-128)<neutral_th)\n",
        "    is_white = (L > 230) & (np.abs(a-128)<neutral_th) & (np.abs(b-128)<neutral_th)\n",
        "    hist = cv2.calcHist([lab],[0,1,2],None,[bins,bins,bins],[0,256,0,256,0,256]).flatten()\n",
        "    black_count = int(is_black.sum()); white_count = int(is_white.sum())\n",
        "    total = hist.sum() + black_count + white_count + 1e-8\n",
        "    h = np.append(hist, [black_count, white_count]).astype(np.float32) / total\n",
        "    return h\n",
        "\n",
        "def hcl_hist(img_np, bins=8, c_max=150.0):\n",
        "    lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2Lab).astype(np.float32)\n",
        "    L,a8,b8 = cv2.split(lab)\n",
        "    a = a8 - 128.0; b = b8 - 128.0\n",
        "    C = np.sqrt(a**2 + b**2)\n",
        "    H = (np.degrees(np.arctan2(b,a)) + 360.0) % 360.0\n",
        "    neutral_th = 5.0\n",
        "    is_black = (L < 25) & (C < neutral_th)\n",
        "    is_white = (L > 230) & (C < neutral_th)\n",
        "    coords = np.stack([L.flatten(), C.flatten(), H.flatten()], axis=-1)\n",
        "    hist, _ = np.histogramdd(coords, bins=(bins,bins,bins), range=((0,256),(0,c_max),(0,360)))\n",
        "    hist = hist.flatten()\n",
        "    black_count = int(is_black.sum()); white_count = int(is_white.sum())\n",
        "    total = hist.sum() + black_count + white_count + 1e-8\n",
        "    h = np.append(hist, [black_count, white_count]).astype(np.float32) / total\n",
        "    return h\n",
        "\n",
        "def compute_histogram(pil_img, color_space, bins=8):\n",
        "    np_img = np.array(pil_img.resize((256,256)))\n",
        "    if color_space == \"rgb512\":\n",
        "        return rgb_hist(np_img, bins=bins)\n",
        "    elif color_space == \"lab514\":\n",
        "        return lab_hist(np_img, bins=bins)\n",
        "    elif color_space == \"hcl514\":\n",
        "        return hcl_hist(np_img, bins=bins)\n",
        "    else:\n",
        "        raise ValueError(f\"unknown color_space {color_space}\")\n",
        "\n",
        "# --- metrics ---\n",
        "def sinkhorn_emd(h1, h2, blur=0.05):\n",
        "    if not HAVE_GEOMLOSS:\n",
        "        return float(np.abs(h1 - h2).mean())\n",
        "    D = h1.shape[0]\n",
        "    x = torch.arange(D, dtype=torch.float32, device=\"cpu\").view(D,1)\n",
        "    a = torch.tensor(h1, dtype=torch.float32).view(1, D)\n",
        "    b = torch.tensor(h2, dtype=torch.float32).view(1, D)\n",
        "    loss = SamplesLoss(\"sinkhorn\", p=2, blur=blur, backend=\"tensorized\")\n",
        "    return float(loss(x, x, a, b).item())\n",
        "\n",
        "def neutral_bin_errors(h_gen, h_tgt, color_space):\n",
        "    if color_space in (\"lab514\",\"hcl514\"):\n",
        "        black_err = float(abs(h_gen[-2] - h_tgt[-2]))\n",
        "        white_err = float(abs(h_gen[-1] - h_tgt[-1]))\n",
        "        return black_err, white_err\n",
        "    return math.nan, math.nan\n",
        "\n",
        "def edge_map(pil_img, size=512, low=100, high=200):\n",
        "    img = np.array(pil_img.resize((size,size)).convert(\"L\"))\n",
        "    e = cv2.Canny(img, low, high)\n",
        "    return (e > 0).astype(np.uint8)\n",
        "\n",
        "def edge_f1(target_edges, pred_edges, tol=1):\n",
        "    kernel = np.ones((2*tol+1,2*tol+1), np.uint8)\n",
        "    target_dil = cv2.dilate(target_edges, kernel, iterations=1)\n",
        "    tp = np.logical_and(pred_edges, target_dil).sum()\n",
        "    fp = np.logical_and(pred_edges, ~target_dil).sum()\n",
        "    fn = np.logical_and(~pred_edges, target_edges).sum()\n",
        "    prec = tp / max(1, tp+fp); rec = tp / max(1, tp+fn)\n",
        "    f1 = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
        "    return float(f1), float(prec), float(rec)\n",
        "\n",
        "def edge_ssim(target_edges, pred_edges):\n",
        "    a = target_edges.astype(np.float32)\n",
        "    b = pred_edges.astype(np.float32)\n",
        "    return float(ssim(a, b, data_range=1.0))\n",
        "\n",
        "def clip_score_openclip(model, preprocess, device, prompt, pil_img):\n",
        "    img_t = preprocess(pil_img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
        "        im = model.encode_image(img_t)\n",
        "    im = F.normalize(im.float(), dim=-1)\n",
        "    with torch.no_grad():\n",
        "        txt = open_clip.tokenize([prompt]).to(device)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
        "            tt = model.encode_text(txt)\n",
        "    tt = F.normalize(tt.float(), dim=-1)\n",
        "    return float((im @ tt.T).squeeze().item())\n",
        "\n",
        "def control_edge_from_path(path, size=512):\n",
        "    pil = Image.open(path).convert(\"RGB\")\n",
        "    gray = ImageOps.grayscale(pil)\n",
        "    gray = ImageOps.autocontrast(gray)\n",
        "    return gray.resize((size,size), Image.BILINEAR).convert(\"RGB\")\n",
        "\n",
        "def palette_bar_figure(h, outpath, title=\"\"):\n",
        "    import matplotlib\n",
        "    matplotlib.use(\"Agg\")\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    plt.figure(figsize=(4,1.2))\n",
        "    plt.bar(np.arange(len(h)), h)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outpath, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def save_ablation_montage(imgs, labels, bars_pngs, outpath, cell=512, pad=12):\n",
        "    cols = 2; rows = 2\n",
        "    W = cols*cell + (cols+1)*pad\n",
        "    H = rows*(cell+90) + (rows+1)*pad\n",
        "    canvas = Image.new(\"RGB\", (W,H), (245,245,245))\n",
        "    draw = ImageDraw.Draw(canvas)\n",
        "    for i,(im,lbl,barpng) in enumerate(zip(imgs, labels, bars_pngs)):\n",
        "        r = i//cols; c = i%cols\n",
        "        x = pad + c*(cell+pad); y = pad + r*(cell+90+pad)\n",
        "        canvas.paste(im.resize((cell,cell), Image.LANCZOS), (x,y))\n",
        "        draw.text((x, y+cell+4), lbl, fill=(0,0,0))\n",
        "        if barpng and os.path.exists(barpng):\n",
        "            bar = Image.open(barpng).convert(\"RGB\").resize((cell,80), Image.BILINEAR)\n",
        "            canvas.paste(bar, (x, y+cell+20))\n",
        "    canvas.save(outpath)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Build pipelines and models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_pipelines(sd_id, controlnet_id, device, hf_cache=None):\n",
        "    cn = ControlNetModel.from_pretrained(controlnet_id, torch_dtype=torch.float16, cache_dir=hf_cache)\n",
        "    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "        sd_id, controlnet=cn, torch_dtype=torch.float16,\n",
        "        safety_checker=None, feature_extractor=None, cache_dir=hf_cache\n",
        "    ).to(device)\n",
        "    pipe.controlnet = pipe.controlnet.to(dtype=torch.float16)\n",
        "    return pipe\n",
        "\n",
        "def build_ip_adapter(pipe, ip_ckpt, image_encoder_path, device, num_tokens=4, embedding_type=\"custom\"):\n",
        "    return IPAdapter(\n",
        "        sd_pipe=pipe, image_encoder_path=image_encoder_path,\n",
        "        ip_ckpt=ip_ckpt, device=device, num_tokens=num_tokens,\n",
        "        embedding_type=embedding_type,\n",
        "    )\n",
        "\n",
        "def build_color_head(ckpt_path, clip_dim, hist_dim, device):\n",
        "    head = ColorHead(clip_dim=clip_dim, hist_dim=hist_dim).to(device).eval()\n",
        "    head.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "    return head\n",
        "\n",
        "def controlnet_knobs(scale):\n",
        "    return dict(\n",
        "        controlnet_conditioning_scale=scale,\n",
        "        control_guidance_start=0.0,\n",
        "        control_guidance_end=1.0,\n",
        "    )"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Load data & models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_manifest = load_manifest(cfg.manifest_csv)\n",
        "df_prompts = pd.read_csv(cfg.prompts_csv)\n",
        "if cfg.limit and len(df_prompts) > cfg.limit:\n",
        "    df_prompts = df_prompts.head(cfg.limit).copy()\n",
        "\n",
        "emb = np.load(cfg.embeddings_npy, mmap_mode=\"r\")\n",
        "hists = np.load(cfg.hists_npy, mmap_mode=\"r\")\n",
        "N, clip_dim = emb.shape\n",
        "hist_dim = hists.shape[1]\n",
        "assert N == len(df_manifest), \"embeddings/hists must align with manifest rows\"\n",
        "\n",
        "pipe = build_pipelines(cfg.sd_id, cfg.controlnet_id, device, hf_cache=cfg.hf_cache)\n",
        "ip_adapter = build_ip_adapter(pipe, cfg.ip_ckpt, cfg.image_encoder_path, device, embedding_type=\"custom\")\n",
        "color_head = build_color_head(cfg.color_head_ckpt, clip_dim=clip_dim, hist_dim=hist_dim, device=device)\n",
        "\n",
        "# CLIPScore model\n",
        "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
        "    \"ViT-H-14\", pretrained=\"laion2b_s32b_b79k\", cache_dir=cfg.hf_cache\n",
        ")\n",
        "clip_model = clip_model.to(device)\n",
        "if device.type == \"cuda\":\n",
        "    clip_model = clip_model.half()\n",
        "clip_model.eval()\n",
        "\n",
        "print(\"Loaded:\")\n",
        "print(\"  manifest:\", len(df_manifest), \"rows\")\n",
        "print(\"  prompts:\", len(df_prompts), \"rows\")\n",
        "print(\"  emb shape:\", emb.shape, \" hist shape:\", hists.shape)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Evaluation loop (ablation + scale grid)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate(cfg, df_manifest, df_prompts, emb, hists, device):\n",
        "    outdir = Path(cfg.outdir)\n",
        "    img_dir = outdir / \"imgs\"; fig_dir = outdir / \"figs\"; csv_dir = outdir / \"csv\"\n",
        "    for d in (img_dir, fig_dir, csv_dir):\n",
        "        ensure_dir(d)\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for ridx, row in df_prompts.iterrows():\n",
        "        idx = int(row[\"idx\"])  # must map to manifest row\n",
        "        prompt = str(row[\"prompt\"])\n",
        "\n",
        "        src_path = df_manifest.iloc[idx][\"file_path\"]\n",
        "        tgt_hist = hists[idx].astype(np.float32)\n",
        "\n",
        "        z = torch.tensor(emb[idx], dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            _, _, c_emb = color_head(z)\n",
        "        c_emb = c_emb.float()\n",
        "        control_pil = control_edge_from_path(src_path, size=cfg.edge_size)\n",
        "\n",
        "        # 2x2 ablation\n",
        "        ablation = [\n",
        "            (\"Text only\", 0.0, None),\n",
        "            (\"+Edges\", 0.0, 1.6),\n",
        "            (\"+Colour\", 0.75, None),\n",
        "            (\"+Both\", 0.75, 1.6),\n",
        "        ]\n",
        "        ablation_imgs, ablation_labels, ablation_bars = [], [], []\n",
        "        for label, ip_s, cn_s in ablation:\n",
        "            ip_adapter.set_scale(ip_s)\n",
        "            kw = {}\n",
        "            if cn_s is not None:\n",
        "                kw.update(dict(image=control_pil))\n",
        "                kw.update(controlnet_knobs(cn_s))\n",
        "            t0 = time.time()\n",
        "            imgs = ip_adapter.generate_from_embeddings(\n",
        "                clip_image_embeds=c_emb.half(),\n",
        "                prompt=prompt, negative_prompt=None,\n",
        "                scale=ip_s, num_samples=1, seed=cfg.seed,\n",
        "                guidance_scale=cfg.cfg_scale, num_inference_steps=cfg.steps,\n",
        "                **kw\n",
        "            )\n",
        "            latency = time.time() - t0\n",
        "            gen = imgs[0].convert(\"RGB\")\n",
        "\n",
        "            gen_hist = compute_histogram(gen, cfg.color_space)\n",
        "            emd = sinkhorn_emd(gen_hist, tgt_hist)\n",
        "            b_err, w_err = neutral_bin_errors(gen_hist, tgt_hist, cfg.color_space)\n",
        "            pred_edges = edge_map(gen, size=cfg.edge_size, low=cfg.canny_low, high=cfg.canny_high)\n",
        "            targ_edges = edge_map(control_pil, size=cfg.edge_size, low=cfg.canny_low, high=cfg.canny_high)\n",
        "            f1, prec, rec = edge_f1(targ_edges, pred_edges, tol=1)\n",
        "            ess = edge_ssim(targ_edges, pred_edges)\n",
        "            cs = clip_score_openclip(clip_model, clip_preprocess, device, prompt, gen)\n",
        "\n",
        "            out_name = f\"{ridx:03d}_{label.replace('+','plus').replace(' ','_')}.png\"\n",
        "            gen_path = img_dir / out_name\n",
        "            gen.save(gen_path)\n",
        "\n",
        "            bar_tgt = fig_dir / f\"{ridx:03d}_tgtbar.png\"\n",
        "            bar_gen = fig_dir / f\"{ridx:03d}_{label.replace('+','plus').replace(' ','_')}_bar.png\"\n",
        "            if not os.path.exists(bar_tgt):\n",
        "                palette_bar_figure(tgt_hist, bar_tgt, title=\"Target palette\")\n",
        "            palette_bar_figure(gen_hist, bar_gen, title=\"Generated palette\")\n",
        "\n",
        "            ablation_imgs.append(gen)\n",
        "            ablation_labels.append(f\"{label}\\nEMD={emd:.4f}  F1={f1:.3f}  CLIP={cs:.3f}\")\n",
        "            ablation_bars.append(str(bar_gen))\n",
        "\n",
        "            rows.append(dict(\n",
        "                prompt_idx=idx, row_id=ridx, mode=label, ip_scale=ip_s,\n",
        "                cn_scale=(cn_s if cn_s is not None else 0.0),\n",
        "                cfg=cfg.cfg_scale, steps=cfg.steps, latency_s=latency,\n",
        "                color_emd=emd, neutral_black_err=b_err, neutral_white_err=w_err,\n",
        "                edge_f1=f1, edge_prec=prec, edge_rec=rec, edge_ssim=ess,\n",
        "                clip_score=cs, image_path=str(gen_path)\n",
        "            ))\n",
        "\n",
        "        montage_path = fig_dir / f\"{ridx:03d}_ablation_montage.png\"\n",
        "        save_ablation_montage(ablation_imgs, ablation_labels, [None]+ablation_bars[1:], montage_path)\n",
        "\n",
        "        # scale grid\n",
        "        for ip_s, cn_s in itertools.product(cfg.ip_scales, cfg.cn_scales):\n",
        "            ip_adapter.set_scale(ip_s)\n",
        "            kw = dict(image=control_pil)\n",
        "            kw.update(controlnet_knobs(cn_s))\n",
        "            t0 = time.time()\n",
        "            imgs = ip_adapter.generate_from_embeddings(\n",
        "                clip_image_embeds=c_emb.half(),\n",
        "                prompt=prompt, negative_prompt=None,\n",
        "                scale=ip_s, num_samples=1, seed=cfg.seed,\n",
        "                guidance_scale=cfg.cfg_scale, num_inference_steps=cfg.steps,\n",
        "                **kw\n",
        "            )\n",
        "            latency = time.time() - t0\n",
        "            gen = imgs[0].convert(\"RGB\")\n",
        "\n",
        "            gen_hist = compute_histogram(gen, cfg.color_space)\n",
        "            emd = sinkhorn_emd(gen_hist, tgt_hist)\n",
        "            b_err, w_err = neutral_bin_errors(gen_hist, tgt_hist, cfg.color_space)\n",
        "            pred_edges = edge_map(gen, size=cfg.edge_size, low=cfg.canny_low, high=cfg.canny_high)\n",
        "            targ_edges = edge_map(control_pil, size=cfg.edge_size, low=cfg.canny_low, high=cfg.canny_high)\n",
        "            f1, prec, rec = edge_f1(targ_edges, pred_edges, tol=1)\n",
        "            ess = edge_ssim(targ_edges, pred_edges)\n",
        "            cs = clip_score_openclip(clip_model, clip_preprocess, device, prompt, gen)\n",
        "\n",
        "            out_name = f\"{ridx:03d}_grid_ip{ip_s}_cn{cn_s}.png\".replace(\".\",\"p\")\n",
        "            gen_path = img_dir / out_name\n",
        "            gen.save(gen_path)\n",
        "\n",
        "            rows.append(dict(\n",
        "                prompt_idx=idx, row_id=ridx, mode=\"grid\",\n",
        "                ip_scale=ip_s, cn_scale=cn_s, cfg=cfg.cfg_scale, steps=cfg.steps,\n",
        "                latency_s=latency, color_emd=emd,\n",
        "                neutral_black_err=b_err, neutral_white_err=w_err,\n",
        "                edge_f1=f1, edge_prec=prec, edge_rec=rec, edge_ssim=ess,\n",
        "                clip_score=cs, image_path=str(gen_path)\n",
        "            ))\n",
        "\n",
        "    out_csv = Path(cfg.outdir)/\"csv\"/\"metrics.csv\"\n",
        "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
        "    return out_csv\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Run\n",
        "This will generate images, montages, and a CSV into `cfg.outdir`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ensure_dir(cfg.outdir)\n",
        "csv_path = evaluate(cfg, df_manifest, df_prompts, emb, hists, device)\n",
        "print(\"\u2705 Done. CSV:\", csv_path)\n",
        "print(\"Images:\", Path(cfg.outdir)/\"imgs\")\n",
        "print(\"Figures:\", Path(cfg.outdir)/\"figs\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) (Optional) Peek at a montage inline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from IPython.display import display\n",
        "figs_dir = Path(cfg.outdir)/\"figs\"\n",
        "montages = sorted(figs_dir.glob(\"*_ablation_montage.png\"))\n",
        "if montages:\n",
        "    display(Image.open(montages[0]))\n",
        "else:\n",
        "    print(\"No montages yet.\")"
      ],
      "outputs": []
    }
  ]
}